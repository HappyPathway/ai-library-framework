#!/usr/bin/env python3
"""
Autodoc Agent for Python Dependencies

This agent scans the project for Python dependencies in requirements.txt files, 
categorizes them, and generates comprehensive documentation by scraping online sources.

It uses the project's AI engine and web scraper utilities to:
1. Identify Python dependencies from requirements.txt files
2. Fetch documentation from official sources
3. Generate markdown documentation categorized by purpose
4. Save documentation to the project

Usage:
    ./agents/autodoc [--output-dir <directory>]
"""

import argparse
import asyncio
import json
import os
import re
import subprocess
import sys
import warnings
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from bs4 import BeautifulSoup
from pydantic import BaseModel, Field

from utils.ai_engine import AIEngine
from utils.logging import setup_logging  # Fixed: changed setup_logger to setup_logging
from utils.schemas.ai import AnthropicSettings
from utils.web_scraper import WebScraper

# Filter warnings from Pydantic
warnings.filterwarnings("ignore", message=r"Field name .* shadows an attribute in parent")


# Add project root to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))


class PackageSummary(BaseModel):
    """Schema for AI-generated package summaries."""
    summary: str = Field(
        description="A concise 2-3 sentence summary of what the package does, its key features, and common use cases"
    )


# Initialize logger
logger = setup_logging('autodoc_agent')  # Fixed: changed setup_logger to setup_logging

# Default output path
DEFAULT_OUTPUT_PATH = Path('./docs/dependencies.md')

# Package categories with their descriptions
PACKAGE_CATEGORIES = {
    'ai': 'AI and machine learning libraries for model interaction and processing',
    'cloud': 'Cloud service integration libraries',
    'web': 'Web services, APIs, and networking',
    'data': 'Data processing, storage, and serialization',
    'testing': 'Testing and quality assurance',
    'dev': 'Development tools and utilities',
    'core': 'Core Python functionality extensions',
    'async': 'Asynchronous programming tools',
    'other': 'Miscellaneous libraries'
}

# Patterns to identify package categories
CATEGORY_PATTERNS = {
    'ai': [
        r'openai', r'anthropic', r'langchain', r'llm', r'google.*ai',
        r'pydantic-ai', r'huggingface', r'transformers', r'tiktoken',
        r'tokenizers', r'ai', r'ml', r'bert', r'gpt', r'claude',
        r'gemini', r'mistral'
    ],
    'cloud': [
        r'google-cloud', r'boto3', r'azure', r'aws', r'gcp',
        r'cloud.*storage', r'secret.*manager', r's3', r'dynamodb'
    ],
    'web': [
        r'requests', r'httpx', r'aiohttp', r'fastapi', r'flask',
        r'django', r'starlette', r'uvicorn', r'werkzeug', r'tornado',
        r'websockets', r'beautifulsoup4', r'bs4', r'selenium'
    ],
    'data': [
        r'pandas', r'numpy', r'scipy', r'matplotlib', r'sqlalchemy',
        r'pydantic', r'marshmallow', r'dataclass', r'redis', r'mongodb',
        r'pymongo', r'psycopg', r'sqlite', r'postgresql', r'mysql'
    ],
    'testing': [
        r'pytest', r'unittest', r'mock', r'coverage', r'tox',
        r'flake8', r'pylint', r'mypy', r'pyright', r'type.*check'
    ],
    'async': [
        r'asyncio', r'aio', r'asynchronous', r'async.*timeout', r'anyio',
        r'trio', r'celery', r'kafka', r'zmq', r'rabbitmq', r'amqp'
    ],
    'dev': [
        r'black', r'isort', r'autopep8', r'pylint', r'sphinx', r'mkdocs',
        r'docker', r'dask', r'tool', r'linting', r'formatter', r'lint'
    ],
    'core': [
        r'typing', r'log', r'dotenv', r'click', r'rich', r'tqdm'
    ]
}


class DependencyInfo:
    """Store information about a Python package dependency."""

    def __init__(self, name: str, version: Optional[str] = None):
        self.name = name
        self.version = version
        self.description = ""
        self.website = ""
        self.category = "other"
        self.docs_url = ""
        self.github_url = ""
        self.summary = ""

    def __str__(self) -> str:
        return f"{self.name} {self.version or ''}"

    def to_dict(self) -> Dict:
        """Convert to dictionary."""
        return {
            "name": self.name,
            "version": self.version,
            "description": self.description,
            "website": self.website,
            "category": self.category,
            "docs_url": self.docs_url,
            "github_url": self.github_url,
            "summary": self.summary
        }

    @classmethod
    def from_dict(cls, data: Dict) -> 'DependencyInfo':
        """Create from dictionary."""
        dep = cls(data["name"], data.get("version"))
        dep.description = data.get("description", "")
        dep.website = data.get("website", "")
        dep.category = data.get("category", "other")
        dep.docs_url = data.get("docs_url", "")
        dep.github_url = data.get("github_url", "")
        dep.summary = data.get("summary", "")
        return dep


class AutodocAgent:
    """Agent for automatic documentation of Python dependencies."""

    def __init__(self, output_path: Path = DEFAULT_OUTPUT_PATH):
        """Initialize the autodoc agent.

        Args:
            output_path: Path where documentation will be saved
        """
        self.output_path = output_path
        self.dependencies: Dict[str, DependencyInfo] = {}
        self.categorized_deps: Dict[str, List[DependencyInfo]] = defaultdict(list)
        self.scraper = WebScraper(rate_limit=1.0)  # Respect rate limits

        # Initialize AIEngine with the correct parameters
        self.ai_engine = AIEngine(
            feature_name="dependency_documentation",
            model_name="google-gla:gemini-2.5-pro-preview-03-25"  # Use Pydantic AI compatible model name
        )

        self.cache_file = Path('./agents/.dependency_cache.json')
        self.cached_info = self._load_cache()

    def _load_cache(self) -> Dict[str, Dict]:
        """Load cached package info from JSON file."""
        if not self.cache_file.exists():
            return {}

        try:
            with open(self.cache_file, 'r') as f:
                return json.load(f)
        except json.JSONDecodeError:
            logger.warning(f"Error reading cache file: {self.cache_file}")
            return {}

    def _save_cache(self) -> None:
        """Save package info to cache file."""
        cache_data = {}
        for name, dep_info in self.dependencies.items():
            cache_data[name] = dep_info.to_dict()

        self.cache_file.parent.mkdir(exist_ok=True)
        with open(self.cache_file, 'w') as f:
            json.dump(cache_data, f, indent=2)

    def find_dependencies(self) -> None:
        """Find Python dependencies in requirements.txt files."""
        logger.info("Finding Python dependencies from requirements.txt files...")

        # Check requirements files
        self._scan_requirements_files()

        logger.info(f"Found {len(self.dependencies)} dependencies")

    def _scan_requirements_files(self) -> None:
        """Scan requirements files for dependencies."""
        requirements_files = list(Path('./').glob('*requirements*.txt'))
        requirements_files.extend(list(Path('./requirements').glob('*.txt')))

        if not requirements_files:
            logger.warning("No requirements.txt files found")
            return

        for req_file in requirements_files:
            logger.info(f"Scanning {req_file}")
            try:
                with open(req_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if not line or line.startswith('#') or line.startswith('-'):
                            continue

                        # Extract package name and version
                        match = re.match(r'^([A-Za-z0-9_.-]+)[=~<>!]?=?([0-9a-zA-Z.-]*)', line)
                        if match:
                            name, version = match.groups()
                            name = name.lower()

                            if name not in self.dependencies:
                                self.dependencies[name] = DependencyInfo(name, version)
                            elif version and not self.dependencies[name].version:
                                self.dependencies[name].version = version
            except Exception as e:
                logger.error(f"Error reading {req_file}: {e}")

    def categorize_dependencies(self) -> None:
        """Categorize dependencies based on patterns and AI analysis."""
        logger.info("Categorizing dependencies...")

        # First pass: use pattern matching
        for name, dep_info in self.dependencies.items():
            for category, patterns in CATEGORY_PATTERNS.items():
                if any(re.search(pattern, name, re.IGNORECASE) for pattern in patterns):
                    dep_info.category = category
                    break

        # Group by category
        for name, dep_info in self.dependencies.items():
            self.categorized_deps[dep_info.category].append(dep_info)

        logger.info("Dependencies categorized")

    async def enrich_dependency_info(self, batch_size: int = 20) -> None:
        """Enrich dependency information with details from PyPI and docs.

        Args:
            batch_size: Number of packages to process in each batch
        """
        logger.info("Enriching dependency information...")
        packages_to_enrich = []
        cached_count = 0

        # Prioritize packages: focus on packages that aren't in the cache
        for name, dep_info in self.dependencies.items():
            if name in self.cached_info:
                # Use cached info
                cached_data = self.cached_info[name]
                self.dependencies[name] = DependencyInfo.from_dict(cached_data)
                cached_count += 1
            else:
                packages_to_enrich.append(name)

        if cached_count > 0:
            logger.info(f"Using cached information for {cached_count} packages")

        if packages_to_enrich:
            logger.info(f"Fetching new information for {len(packages_to_enrich)} packages")

            # Process in batches with specified size
            for i in range(0, len(packages_to_enrich), batch_size):
                batch = packages_to_enrich[i:i + batch_size]
                await asyncio.gather(*(self._enrich_single_dependency(name) for name in batch))

                # Update cache regularly
                self._save_cache()
        else:
            logger.info("All packages using cached information - use --refresh to force update")

        logger.info("Dependency information enriched")

    async def _enrich_single_dependency(self, name: str) -> None:
        """Enrich a single dependency with information from PyPI and docs."""
        dep_info = self.dependencies[name]
        logger.info(f"Enriching info for {name}")

        try:
            # Get package info from PyPI
            pypi_info = await self._get_pypi_info(name)
            if pypi_info:
                dep_info.description = pypi_info.get('summary', '')
                dep_info.website = pypi_info.get('project_url', '')
                dep_info.docs_url = pypi_info.get('docs_url', '')
                dep_info.github_url = pypi_info.get('github_url', '')

            # Try to fetch documentation summary using AI
            if dep_info.docs_url:
                dep_info.summary = await self._fetch_docs_summary(name, dep_info.docs_url)
            elif dep_info.website:
                dep_info.summary = await self._fetch_docs_summary(name, dep_info.website)

        except Exception as e:
            logger.error(f"Error enriching {name}: {e}")

    async def _get_pypi_info(self, package_name: str) -> Optional[Dict]:
        """Get package information from PyPI."""
        pypi_url = f"https://pypi.org/pypi/{package_name}/json"

        try:
            # Remove the timeout parameter that was causing the error
            response = await asyncio.to_thread(
                self.scraper.get, pypi_url
            )

            if not response or response.status_code != 200:
                return None

            data = response.json()
            info = data.get('info', {})

            project_urls = info.get('project_urls', {})

            result = {
                'summary': info.get('summary', ''),
                'project_url': info.get('project_url', info.get('home_page', '')),
                'docs_url': project_urls.get('Documentation', ''),
                'github_url': next((url for name, url in project_urls.items()
                                    if 'github' in name.lower() or 'repository' in name.lower()), '')
            }

            return result

        except Exception as e:
            logger.error(f"Error fetching PyPI info for {package_name}: {e}")
            return None

    async def _fetch_docs_summary(self, package_name: str, url: str) -> str:
        """Fetch and summarize documentation using AI."""
        try:
            # Try to fetch content
            response = await asyncio.to_thread(
                self.scraper.get, url
            )

            if not response or response.status_code != 200:
                return ""

            # Parse HTML
            try:
                soup = BeautifulSoup(response.content, 'html.parser')
                if not soup:
                    return ""
            except Exception as e:
                logger.error(f"Error parsing HTML from {url}: {e}")
                return ""

            # Extract relevant text
            text = []
            for tag in soup.select("p, h1, h2, h3, li"):
                tag_text = tag.get_text(strip=True)
                if len(tag_text) > 20:  # Skip very short fragments
                    text.append(tag_text)

            # Limit text to avoid token limits
            content = "\n".join(text[:20])
            if not content:
                return ""

            # Generate summary with AI using our defined schema
            prompt = f"""
            Summarize the Python package '{package_name}' based on the following documentation excerpt.
            Focus on:
            1. What the package does
            2. Key features and capabilities
            3. Common use cases
            
            Keep the summary concise (2-3 sentences).
            
            Documentation content:
            {content}
            """

            # Max retries for 503 errors
            max_retries = 3
            retry_count = 0
            base_delay = 1.0

            while retry_count <= max_retries:
                try:
                    # Get result from AIEngine
                    result = await self.ai_engine.generate(
                        prompt=prompt,
                        temperature=0.3  # Lower temperature for more consistent summaries
                    )

                    # If we reach here, successful generation
                    # Handle different return types
                    if isinstance(result, str):
                        # If result is a string, use it directly
                        return result
                    elif hasattr(result, 'summary'):
                        # If result has a summary attribute (PackageSummary), use that
                        return result.summary
                    elif hasattr(result, 'output') and result.output:
                        # If result has an output attribute, use that
                        return str(result.output)
                    elif hasattr(result, 'content') and result.content:
                        # If result has a content attribute, use that
                        return str(result.content)
                    elif result:
                        # Last resort: stringify the result, but filter out AgentRunResult representation
                        result_str = str(result)
                        if result_str.startswith('AgentRunResult'):
                            # Try to extract content from inside parentheses
                            import re
                            match = re.search(r'output=[\'"]?(.*?)[\'"]?\)', result_str)
                            if match:
                                return match.group(1)
                            # If we can't extract anything meaningful, return empty string
                            return ""
                        return result_str
                    else:
                        return ""

                except Exception as inner_e:
                    error_str = str(inner_e)

                    # Check if this is a service unavailable error (503)
                    if "503" in error_str and "service is currently unavailable" in error_str.lower() and retry_count < max_retries:
                        # Exponential backoff
                        delay = base_delay * (2 ** retry_count)
                        retry_count += 1
                        logger.warning(f"Gemini API unavailable (503) for {package_name}, retrying in {delay}s (attempt {retry_count}/{max_retries})")
                        await asyncio.sleep(delay)
                        continue
                    else:
                        # Other error or max retries reached
                        logger.error(f"Error processing AI response for {package_name}: {inner_e}")
                        return ""

            # If we've exhausted retries
            logger.error(f"Maximum retries reached when summarizing {package_name}")
            return ""

        except Exception as e:
            logger.error(f"Error fetching docs for {package_name}: {e}")
            return ""

    async def generate_documentation(self) -> None:
        """Generate comprehensive markdown documentation."""
        logger.info(f"Generating documentation to {self.output_path}")

        # Create parent directory if it doesn't exist
        self.output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(self.output_path, 'w') as f:
            f.write("# Python Dependencies\n\n")
            f.write("This document provides an overview of the Python dependencies used in this project.\n\n")

            # Create table of contents
            f.write("## Table of Contents\n\n")
            for category in self.categorized_deps:
                if self.categorized_deps[category]:
                    category_name = category.capitalize()
                    category_desc = PACKAGE_CATEGORIES.get(category, "")
                    f.write(f"- [{category_name}](#category-{category}): {category_desc}\n")
            f.write("\n")

            # Write each category
            for category, deps in self.categorized_deps.items():
                if not deps:
                    continue

                category_name = category.capitalize()
                category_desc = PACKAGE_CATEGORIES.get(category, "")

                f.write(f"## <a name='category-{category}'></a> {category_name} Packages\n\n")
                f.write(f"{category_desc}\n\n")

                # Sort by package name
                deps.sort(key=lambda x: x.name)

                for dep in deps:
                    version_text = f" ({dep.version})" if dep.version else ""
                    f.write(f"### {dep.name}{version_text}\n\n")

                    if dep.description:
                        f.write(f"{dep.description}\n\n")

                    if dep.summary:
                        f.write(f"{dep.summary}\n\n")

                    # Links
                    links = []
                    if dep.website:
                        links.append(f"[Website]({dep.website})")
                    if dep.docs_url:
                        links.append(f"[Documentation]({dep.docs_url})")
                    if dep.github_url:
                        links.append(f"[GitHub]({dep.github_url})")

                    if links:
                        f.write("**Links:** " + " | ".join(links) + "\n\n")

                    # Add a line to separate packages
                    f.write("---\n\n")

            # Add a footer with generation info
            f.write("\n\n---\n\n")
            f.write("*This document was automatically generated by the Autodoc Agent.*\n")

        logger.info(f"Documentation saved to {self.output_path}")


async def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Generate documentation for Python dependencies from requirements.txt files")
    parser.add_argument('--output-dir', type=str, default='./docs',
                        help='Output directory for documentation')
    parser.add_argument('--refresh', action='store_true',
                        help='Force refresh of all dependency data')
    parser.add_argument('--model', type=str,
                        default="google-gla:gemini-2.5-pro-preview-03-25",
                        help='AI model to use (e.g. google-gla:gemini-2.5-pro-preview-03-25, openai:gpt-4)')
    parser.add_argument('--skip-ai', action='store_true',
                        help='Skip AI-based documentation enrichment (faster)')
    parser.add_argument('--batch-size', type=int, default=20,
                        help='Number of packages to process in each batch')
    parser.add_argument('--limit', type=int, default=0,
                        help='Limit number of packages to process (0 = no limit)')
    parser.add_argument('--filter', type=str, default='',
                        help='Filter packages by name (case-insensitive substring match)')
    parser.add_argument('--category', type=str, default='',
                        help='Filter packages by category')

    args = parser.parse_args()
    output_path = Path(args.output_dir) / 'dependencies.md'

    # Initialize agent with specified model
    agent = AutodocAgent(output_path)
    if args.model and agent.ai_engine.model_name != args.model:
        agent.ai_engine.model_name = args.model
        logger.info(f"Using model: {args.model}")

    agent.find_dependencies()
    agent.categorize_dependencies()

    # Clear cache if refresh is requested
    if args.refresh:
        agent.cached_info = {}
        if os.path.exists(agent.cache_file):
            os.remove(agent.cache_file)
            logger.info(f"Removed cache file {agent.cache_file}")
        logger.info("Cache cleared - forcing refresh of all dependency information")

    # Apply filters if specified
    if args.filter or args.category or args.limit:
        original_count = len(agent.dependencies)

        # Apply name filter
        if args.filter:
            filter_str = args.filter.lower()
            filtered_deps = {
                name: info for name, info in agent.dependencies.items()
                if filter_str in name.lower()
            }
            agent.dependencies = filtered_deps
            logger.info(f"Filtered packages by name '{args.filter}': {len(agent.dependencies)}/{original_count}")

        # Apply category filter
        if args.category:
            category = args.category.lower()
            filtered_deps = {
                name: info for name, info in agent.dependencies.items()
                if info.category.lower() == category
            }
            agent.dependencies = filtered_deps
            logger.info(f"Filtered packages by category '{args.category}': {len(agent.dependencies)}/{original_count}")

        # Apply limit
        if args.limit > 0 and len(agent.dependencies) > args.limit:
            # Convert to list, limit, then convert back to dict
            deps_list = list(agent.dependencies.items())[:args.limit]
            agent.dependencies = dict(deps_list)
            logger.info(f"Limited packages to {args.limit}/{original_count}")

    if not args.skip_ai:
        await agent.enrich_dependency_info(batch_size=args.batch_size)
    else:
        logger.info("Skipping AI-based documentation enrichment")

    await agent.generate_documentation()


if __name__ == "__main__":
    asyncio.run(main())
