{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b11801",
   "metadata": {},
   "source": [
    "# Introduction to Automated Prompt Optimization\n",
    "\n",
    "This notebook provides an introduction to using the adaptive learning features of our prompt optimization framework. We'll explore:\n",
    "\n",
    "1. Setting up a PromptLibrary with sample templates\n",
    "2. Creating a PerformanceAnalyzer to track template usage\n",
    "3. Using the AdaptiveLearningManager to optimize prompts\n",
    "4. Visualizing the optimization process and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5baff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core components\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Import our framework components\n",
    "from ailf.cognition.prompt_library import PromptLibrary\n",
    "from ailf.schemas.prompt_engineering import PromptTemplateV1, PromptLibraryConfig\n",
    "from ailf.feedback.performance_analyzer import PerformanceAnalyzer\n",
    "from ailf.feedback.adaptive_learning_manager import AdaptiveLearningManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d821464",
   "metadata": {},
   "source": [
    "## Setup: Creating a Sample Prompt Library\n",
    "\n",
    "First, let's create a temporary directory and set up a PromptLibrary with some sample templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory for our prompt templates\n",
    "import tempfile\n",
    "prompt_library_path = tempfile.mkdtemp()\n",
    "print(f\"Created temporary directory for templates: {prompt_library_path}\")\n",
    "\n",
    "# Initialize configuration for the prompt library\n",
    "config = PromptLibraryConfig(\n",
    "    library_path=prompt_library_path,\n",
    "    default_prompt_id=\"weather_query\",\n",
    "    auto_save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf144601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample templates\n",
    "templates = [\n",
    "    {\n",
    "        \"template_id\": \"weather_query\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"A template for querying weather information\",\n",
    "        \"system_prompt\": \"You are a helpful weather assistant.\",\n",
    "        \"user_prompt_template\": \"What's the weather like in {location} today?\",\n",
    "        \"placeholders\": [\"location\"],\n",
    "        \"tags\": [\"weather\", \"query\"],\n",
    "        \"created_at\": time.time()\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": \"news_summary\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"A template for summarizing news articles\",\n",
    "        \"system_prompt\": \"You are a helpful news summarizer.\",\n",
    "        \"user_prompt_template\": \"Summarize this news article: {article}\",\n",
    "        \"placeholders\": [\"article\"],\n",
    "        \"tags\": [\"news\", \"summary\"],\n",
    "        \"created_at\": time.time()\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": \"product_recommendation\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"A template for recommending products\",\n",
    "        \"system_prompt\": \"You are a product recommendation assistant.\",\n",
    "        \"user_prompt_template\": \"Recommend products for {category} that match {preferences}\",\n",
    "        \"placeholders\": [\"category\", \"preferences\"],\n",
    "        \"tags\": [\"products\", \"recommendations\"],\n",
    "        \"created_at\": time.time()\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save templates to JSON files\n",
    "for template_data in templates:\n",
    "    filename = f\"{template_data['template_id']}_v{template_data['version']}.json\"\n",
    "    filepath = os.path.join(prompt_library_path, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(template_data, f, indent=2)\n",
    "    print(f\"Created template file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6180363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PromptLibrary\n",
    "prompt_library = PromptLibrary(config)\n",
    "\n",
    "# List available templates\n",
    "print(\"Available templates:\")\n",
    "for template_id in prompt_library.list_template_ids():\n",
    "    template = prompt_library.get_template(template_id)\n",
    "    print(f\"- {template.template_id} (v{template.version}): {template.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b6ac5",
   "metadata": {},
   "source": [
    "## Creating a Mock Performance Analyzer\n",
    "\n",
    "Now, let's create a mock PerformanceAnalyzer that provides simulated performance data for our templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple mock PerformanceAnalyzer\n",
    "class MockPerformanceAnalyzer(PerformanceAnalyzer):\n",
    "    \"\"\"A mock performance analyzer for demonstration purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, mock_data=None):\n",
    "        \"\"\"Initialize with optional mock data.\"\"\"\n",
    "        self.mock_data = mock_data or {}\n",
    "        \n",
    "    def analyze_prompt_success(self):\n",
    "        \"\"\"Return mock prompt analysis data.\"\"\"\n",
    "        return self.mock_data.get(\"prompt_analysis\", {})\n",
    "        \n",
    "    def get_general_metrics(self):\n",
    "        \"\"\"Return mock general metrics.\"\"\"\n",
    "        return self.mock_data.get(\"general_metrics\", {})\n",
    "        \n",
    "    def find_prompt_correlations(self):\n",
    "        \"\"\"Return mock correlations.\"\"\"\n",
    "        return self.mock_data.get(\"correlations\", {})\n",
    "\n",
    "# Create mock performance data\n",
    "mock_data = {\n",
    "    \"prompt_analysis\": {\n",
    "        \"weather_query\": {\n",
    "            \"total_uses\": 100,\n",
    "            \"successful_outcomes\": 60,\n",
    "            \"error_count\": 40,\n",
    "            \"average_feedback_score\": 0.3,\n",
    "            \"error_rate\": 0.4,  # High error rate (>30%)\n",
    "            \"success_rate\": 0.6\n",
    "        },\n",
    "        \"news_summary\": {\n",
    "            \"total_uses\": 50,\n",
    "            \"successful_outcomes\": 45,\n",
    "            \"error_count\": 5,\n",
    "            \"average_feedback_score\": 0.8,\n",
    "            \"error_rate\": 0.1,\n",
    "            \"success_rate\": 0.9\n",
    "        },\n",
    "        \"product_recommendation\": {\n",
    "            \"total_uses\": 75,\n",
    "            \"successful_outcomes\": 30,\n",
    "            \"error_count\": 45,\n",
    "            \"average_feedback_score\": 0.1,  # Low feedback score\n",
    "            \"error_rate\": 0.6,\n",
    "            \"success_rate\": 0.4\n",
    "        }\n",
    "    },\n",
    "    \"general_metrics\": {\n",
    "        \"total_interactions\": 225,\n",
    "        \"successful_interactions\": 135,\n",
    "        \"failed_interactions\": 90,\n",
    "        \"average_response_time\": 2.3\n",
    "    },\n",
    "    \"correlations\": {\n",
    "        \"prompt_success_vs_length\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the performance analyzer\n",
    "performance_analyzer = MockPerformanceAnalyzer(mock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569692e",
   "metadata": {},
   "source": [
    "## Setting Up the AdaptiveLearningManager\n",
    "\n",
    "Now let's set up the AdaptiveLearningManager to use our mock performance analyzer and prompt library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration for the AdaptiveLearningManager\n",
    "learning_config = {\n",
    "    \"error_rate_threshold\": 0.3,       # Consider error rates above 30% as high\n",
    "    \"feedback_optimization_threshold\": 0.5,  # Consider feedback scores below 0.5 as low\n",
    "    \"auto_optimize_prompts\": True,     # Enable automatic optimization\n",
    "    \"use_ai_for_improvements\": False,  # Use rule-based improvements (no AI engine here)\n",
    "    \"feedback_suggestion_threshold\": 0.3  # Threshold for suggesting improvements based on feedback\n",
    "}\n",
    "\n",
    "# Initialize the AdaptiveLearningManager\n",
    "learning_manager = AdaptiveLearningManager(\n",
    "    performance_analyzer=performance_analyzer,\n",
    "    prompt_library=prompt_library,\n",
    "    config=learning_config\n",
    ")\n",
    "\n",
    "print(\"AdaptiveLearningManager initialized with configuration:\")\n",
    "print(f\"- Error rate threshold: {learning_config['error_rate_threshold']}\")\n",
    "print(f\"- Feedback optimization threshold: {learning_config['feedback_optimization_threshold']}\")\n",
    "print(f\"- Auto-optimize prompts: {learning_config['auto_optimize_prompts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fa788",
   "metadata": {},
   "source": [
    "## Analyzing Current Performance\n",
    "\n",
    "Let's examine the performance data for our prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance analysis data\n",
    "prompt_analysis = performance_analyzer.analyze_prompt_success()\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "analysis_df = pd.DataFrame.from_dict(\n",
    "    {k: {kk: vv for kk, vv in v.items() if kk in ['error_rate', 'success_rate', 'average_feedback_score']} \n",
    "     for k, v in prompt_analysis.items()},\n",
    "    orient='index'\n",
    ")\n",
    "\n",
    "# Display the performance metrics\n",
    "print(\"Current Performance Metrics:\")\n",
    "display(analysis_df)\n",
    "\n",
    "# Create a visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "analysis_df.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Prompt Template Performance Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axhline(y=learning_config['error_rate_threshold'], color='r', linestyle='--', label='Error Threshold')\n",
    "ax.axhline(y=learning_config['feedback_optimization_threshold'], color='g', linestyle='--', label='Feedback Threshold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440df8e",
   "metadata": {},
   "source": [
    "## Running the Adaptation Cycle\n",
    "\n",
    "Now, let's run the adaptation cycle to optimize our prompt templates based on the performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run the adaptation cycle asynchronously\n",
    "async def run_adaptation_cycle():\n",
    "    # Get the initial state of templates\n",
    "    print(\"Initial templates:\")\n",
    "    initial_templates = {}\n",
    "    for template_id in prompt_library.list_template_ids():\n",
    "        template = prompt_library.get_template(template_id)\n",
    "        initial_templates[template_id] = template\n",
    "        print(f\"- {template_id} (v{template.version}): {template.user_prompt_template}\")\n",
    "    \n",
    "    print(\"\\nRunning optimization cycle...\")\n",
    "    # Run the adaptive learning cycle\n",
    "    cycle_results = await learning_manager.run_learning_cycle(auto_optimize=True)\n",
    "    \n",
    "    # Print optimization results\n",
    "    print(\"\\nOptimization cycle completed:\")\n",
    "    print(f\"- Timestamp: {time.ctime(cycle_results.get('timestamp'))}\")\n",
    "    print(f\"- Cycle ID: {cycle_results.get('cycle_id')}\")\n",
    "    \n",
    "    # Check for optimized prompts\n",
    "    optimized_prompts = cycle_results.get('optimized_prompts', [])\n",
    "    print(f\"\\nOptimized {len(optimized_prompts)} templates\")\n",
    "    \n",
    "    # Display the updated templates\n",
    "    print(\"\\nUpdated templates:\")\n",
    "    for template_id in prompt_library.list_template_ids():\n",
    "        template = prompt_library.get_template(template_id)\n",
    "        initial = initial_templates.get(template_id)\n",
    "        if template.version > initial.version:\n",
    "            print(f\"- {template_id} (v{template.version}, was v{initial.version}):\")\n",
    "            print(f\"  Before: {initial.user_prompt_template}\")\n",
    "            print(f\"  After:  {template.user_prompt_template}\")\n",
    "            if template.version_notes:\n",
    "                print(f\"  Notes:  {template.version_notes}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"- {template_id} (unchanged at v{template.version})\")\n",
    "    \n",
    "    return cycle_results\n",
    "\n",
    "# Run the adaptation cycle and get results\n",
    "cycle_results = await run_adaptation_cycle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc0acb",
   "metadata": {},
   "source": [
    "## Examining Optimization History\n",
    "\n",
    "Let's check the optimization history to see what changes were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimization history\n",
    "optimization_history = learning_manager.get_optimization_history()\n",
    "\n",
    "# Display the optimization history\n",
    "print(f\"Found {len(optimization_history)} optimization records:\")\n",
    "for i, record in enumerate(optimization_history):\n",
    "    print(f\"\\nOptimization {i+1}:\")\n",
    "    print(f\"- Template: {record.get('template_id')}\")\n",
    "    print(f\"- Version: {record.get('original_version')} → {record.get('new_version')}\")\n",
    "    print(f\"- Timestamp: {time.ctime(record.get('timestamp'))}\")\n",
    "    print(f\"- Changes: {record.get('changes', 'No change notes')}\")\n",
    "    \n",
    "    # If metrics are available, show them\n",
    "    metrics = record.get('metrics', {})\n",
    "    if metrics:\n",
    "        print(\"- Metrics that triggered optimization:\")\n",
    "        for k, v in metrics.items():\n",
    "            if isinstance(v, (int, float)):\n",
    "                print(f\"  - {k}: {v:.2f}\")\n",
    "            else:\n",
    "                print(f\"  - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2267d8",
   "metadata": {},
   "source": [
    "## Viewing Template Version History\n",
    "\n",
    "Let's examine the version history of one of the optimized templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display template version history\n",
    "def display_template_history(template_id):\n",
    "    history = prompt_library.get_version_history(template_id)\n",
    "    \n",
    "    print(f\"Version history for template '{template_id}':\")\n",
    "    for version in history:\n",
    "        print(f\"\\nVersion {version.version}:\")\n",
    "        print(f\"- User prompt: {version.user_prompt_template}\")\n",
    "        print(f\"- System prompt: {version.system_prompt}\")\n",
    "        \n",
    "        # Display optimization metadata if available\n",
    "        if hasattr(version, 'updated_by_component') and version.updated_by_component:\n",
    "            print(f\"- Updated by: {version.updated_by_component}\")\n",
    "        if hasattr(version, 'updated_at') and version.updated_at:\n",
    "            print(f\"- Updated at: {time.ctime(version.updated_at)}\")\n",
    "        if hasattr(version, 'optimization_source') and version.optimization_source:\n",
    "            print(f\"- Optimization source: {version.optimization_source}\")\n",
    "        if hasattr(version, 'version_notes') and version.version_notes:\n",
    "            print(f\"- Version notes: {version.version_notes}\")\n",
    "\n",
    "# Check the history of the weather_query template\n",
    "display_template_history(\"weather_query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c4228",
   "metadata": {},
   "source": [
    "## Running Another Optimization Cycle\n",
    "\n",
    "Let's run another optimization cycle with updated performance data to see continuous optimization in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the mock performance data to simulate changes after first optimization\n",
    "performance_analyzer.mock_data[\"prompt_analysis\"][\"weather_query\"][\"error_rate\"] = 0.35  # Still high error rate\n",
    "performance_analyzer.mock_data[\"prompt_analysis\"][\"product_recommendation\"][\"error_rate\"] = 0.25  # Improved but still needs work\n",
    "performance_analyzer.mock_data[\"prompt_analysis\"][\"news_summary\"][\"average_feedback_score\"] = 0.45  # Now below threshold\n",
    "\n",
    "# Run another adaptation cycle\n",
    "print(\"Running second optimization cycle with updated performance data...\\n\")\n",
    "second_cycle_results = await run_adaptation_cycle()\n",
    "\n",
    "# Display optimization history after second cycle\n",
    "print(\"\\nUpdated optimization history:\")\n",
    "optimization_history = learning_manager.get_optimization_history()\n",
    "print(f\"Total optimizations performed: {len(optimization_history)}\")\n",
    "\n",
    "# Display a summary of all template versions\n",
    "print(\"\\nFinal template versions:\")\n",
    "for template_id in prompt_library.list_template_ids():\n",
    "    history = prompt_library.get_version_history(template_id)\n",
    "    latest = prompt_library.get_template(template_id)\n",
    "    print(f\"- {template_id}: {len(history)} versions, latest is v{latest.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f5134",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. Setting up a PromptLibrary with sample templates\n",
    "2. Creating a mock PerformanceAnalyzer with simulated performance data\n",
    "3. Using the AdaptiveLearningManager to automatically optimize underperforming templates\n",
    "4. Running multiple optimization cycles to see continuous improvement\n",
    "5. Viewing the history and evolution of templates over time\n",
    "\n",
    "This demonstrates the end-to-end workflow for automated prompt optimization within the framework."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
