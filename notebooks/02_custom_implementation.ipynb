{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc933fc6",
   "metadata": {},
   "source": [
    "# Custom Implementation Example: Extending the Prompt Optimization Framework\n",
    "\n",
    "This notebook demonstrates how to extend the framework with custom implementations:\n",
    "\n",
    "1. Creating a custom PerformanceAnalyzer with specific metrics\n",
    "2. Implementing a custom AdaptiveLearningManager with domain-specific optimization strategies\n",
    "3. Creating a specialized PromptLibrary with enhanced capabilities\n",
    "4. Building a complete end-to-end workflow for your specific use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312854e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core components\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "# Import our framework components\n",
    "from ailf.cognition.prompt_library import PromptLibrary\n",
    "from ailf.schemas.prompt_engineering import PromptTemplateV1, PromptLibraryConfig\n",
    "from ailf.feedback.performance_analyzer import PerformanceAnalyzer\n",
    "from ailf.feedback.adaptive_learning_manager import AdaptiveLearningManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d202e4",
   "metadata": {},
   "source": [
    "## 1. Creating a Custom PerformanceAnalyzer \n",
    "\n",
    "Let's create a specialized performance analyzer that focuses on specific metrics relevant to a customer service chatbot scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6712e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerServicePerformanceAnalyzer(PerformanceAnalyzer):\n",
    "    \"\"\"\n",
    "    A specialized performance analyzer for customer service chatbot scenarios.\n",
    "    It tracks additional metrics like:\n",
    "    - Resolution rate (percentage of conversations that resolved the issue)\n",
    "    - Customer satisfaction scores\n",
    "    - Handoff rate (percentage of conversations handed to human agents)\n",
    "    - Response time metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interaction_data=None):\n",
    "        \"\"\"Initialize with optional interaction data.\"\"\"\n",
    "        self.interaction_data = interaction_data or []\n",
    "        self.metrics_cache = {}  # Cache for computed metrics\n",
    "        \n",
    "    def add_interaction(self, interaction_data):\n",
    "        \"\"\"Add a new interaction to the dataset.\"\"\"\n",
    "        self.interaction_data.append(interaction_data)\n",
    "        # Clear cache since data has changed\n",
    "        self.metrics_cache = {}\n",
    "    \n",
    "    def analyze_prompt_success(self):\n",
    "        \"\"\"Analyze success metrics for each prompt template used in interactions.\"\"\"\n",
    "        if \"prompt_analysis\" in self.metrics_cache:\n",
    "            return self.metrics_cache[\"prompt_analysis\"]\n",
    "            \n",
    "        prompt_metrics = {}\n",
    "        \n",
    "        for interaction in self.interaction_data:\n",
    "            template_id = interaction.get(\"template_id\")\n",
    "            if not template_id:\n",
    "                continue\n",
    "                \n",
    "            # Ensure template exists in the metrics dict\n",
    "            if template_id not in prompt_metrics:\n",
    "                prompt_metrics[template_id] = {\n",
    "                    \"total_uses\": 0,\n",
    "                    \"successful_outcomes\": 0,\n",
    "                    \"error_count\": 0,\n",
    "                    \"satisfaction_scores\": [],\n",
    "                    \"response_times\": [],\n",
    "                    \"resolution_count\": 0,\n",
    "                    \"handoff_count\": 0\n",
    "                }\n",
    "            \n",
    "            # Update metrics\n",
    "            prompt_metrics[template_id][\"total_uses\"] += 1\n",
    "            \n",
    "            if interaction.get(\"error\"):\n",
    "                prompt_metrics[template_id][\"error_count\"] += 1\n",
    "                \n",
    "            if interaction.get(\"successful\", False):\n",
    "                prompt_metrics[template_id][\"successful_outcomes\"] += 1\n",
    "                \n",
    "            if interaction.get(\"resolved\", False):\n",
    "                prompt_metrics[template_id][\"resolution_count\"] += 1\n",
    "                \n",
    "            if interaction.get(\"handoff\", False):\n",
    "                prompt_metrics[template_id][\"handoff_count\"] += 1\n",
    "                \n",
    "            if \"satisfaction_score\" in interaction:\n",
    "                prompt_metrics[template_id][\"satisfaction_scores\"].append(interaction[\"satisfaction_score\"])\n",
    "                \n",
    "            if \"response_time\" in interaction:\n",
    "                prompt_metrics[template_id][\"response_times\"].append(interaction[\"response_time\"])\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        for template_id, metrics in prompt_metrics.items():\n",
    "            total = metrics[\"total_uses\"]\n",
    "            if total > 0:\n",
    "                # Calculate rates\n",
    "                metrics[\"error_rate\"] = metrics[\"error_count\"] / total\n",
    "                metrics[\"success_rate\"] = metrics[\"successful_outcomes\"] / total\n",
    "                metrics[\"resolution_rate\"] = metrics[\"resolution_count\"] / total\n",
    "                metrics[\"handoff_rate\"] = metrics[\"handoff_count\"] / total\n",
    "                \n",
    "                # Calculate averages\n",
    "                if metrics[\"satisfaction_scores\"]:\n",
    "                    metrics[\"average_satisfaction\"] = sum(metrics[\"satisfaction_scores\"]) / len(metrics[\"satisfaction_scores\"])\n",
    "                else:\n",
    "                    metrics[\"average_satisfaction\"] = None\n",
    "                    \n",
    "                if metrics[\"response_times\"]:\n",
    "                    metrics[\"average_response_time\"] = sum(metrics[\"response_times\"]) / len(metrics[\"response_times\"])\n",
    "                else:\n",
    "                    metrics[\"average_response_time\"] = None\n",
    "        \n",
    "        # Cache the results\n",
    "        self.metrics_cache[\"prompt_analysis\"] = prompt_metrics\n",
    "        return prompt_metrics\n",
    "    \n",
    "    def get_customer_satisfaction_metrics(self):\n",
    "        \"\"\"Calculate customer satisfaction metrics across all interactions.\"\"\"\n",
    "        satisfaction_scores = []\n",
    "        template_satisfaction = {}\n",
    "        \n",
    "        for interaction in self.interaction_data:\n",
    "            if \"satisfaction_score\" not in interaction:\n",
    "                continue\n",
    "                \n",
    "            score = interaction[\"satisfaction_score\"]\n",
    "            satisfaction_scores.append(score)\n",
    "            \n",
    "            # Track by template\n",
    "            template_id = interaction.get(\"template_id\")\n",
    "            if template_id:\n",
    "                if template_id not in template_satisfaction:\n",
    "                    template_satisfaction[template_id] = []\n",
    "                template_satisfaction[template_id].append(score)\n",
    "        \n",
    "        # Calculate overall satisfaction\n",
    "        overall_avg = sum(satisfaction_scores) / len(satisfaction_scores) if satisfaction_scores else None\n",
    "        \n",
    "        # Calculate per-template satisfaction\n",
    "        template_avg = {}\n",
    "        for template_id, scores in template_satisfaction.items():\n",
    "            template_avg[template_id] = sum(scores) / len(scores) if scores else None\n",
    "            \n",
    "        return {\n",
    "            \"overall_satisfaction\": overall_avg,\n",
    "            \"template_satisfaction\": template_avg,\n",
    "            \"satisfaction_count\": len(satisfaction_scores),\n",
    "            \"distribution\": {\n",
    "                \"excellent (4-5)\": sum(1 for s in satisfaction_scores if s >= 4),\n",
    "                \"good (3-4)\": sum(1 for s in satisfaction_scores if 3 <= s < 4),\n",
    "                \"fair (2-3)\": sum(1 for s in satisfaction_scores if 2 <= s < 3),\n",
    "                \"poor (1-2)\": sum(1 for s in satisfaction_scores if 1 <= s < 2),\n",
    "                \"very poor (0-1)\": sum(1 for s in satisfaction_scores if s < 1),\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create some sample interaction data for a customer service chatbot scenario\n",
    "interaction_data = [\n",
    "    # Greeting template interactions\n",
    "    *[{\n",
    "        \"template_id\": \"greeting\", \n",
    "        \"successful\": True,\n",
    "        \"error\": False,\n",
    "        \"resolved\": False,\n",
    "        \"handoff\": False,\n",
    "        \"satisfaction_score\": 4.5 if i % 10 != 0 else 3.0,\n",
    "        \"response_time\": 0.8 + (i % 5) * 0.1\n",
    "    } for i in range(50)],\n",
    "    \n",
    "    # Order status template interactions\n",
    "    *[{\n",
    "        \"template_id\": \"order_status\",\n",
    "        \"successful\": i % 4 != 0,\n",
    "        \"error\": i % 4 == 0,\n",
    "        \"resolved\": i % 3 == 0,\n",
    "        \"handoff\": i % 5 == 0,\n",
    "        \"satisfaction_score\": 3.0 + (i % 10) * 0.2,\n",
    "        \"response_time\": 1.5 + (i % 5) * 0.3\n",
    "    } for i in range(40)],\n",
    "    \n",
    "    # Return policy template interactions\n",
    "    *[{\n",
    "        \"template_id\": \"return_policy\",\n",
    "        \"successful\": i % 2 == 0,  # 50% success rate\n",
    "        \"error\": i % 3 == 0,  # 33% error rate\n",
    "        \"resolved\": i % 2 == 0,  # 50% resolution rate  \n",
    "        \"handoff\": i % 3 == 0,  # 33% handoff rate\n",
    "        \"satisfaction_score\": 2.0 + (i % 5) * 0.5,  # Lower satisfaction\n",
    "        \"response_time\": 2.0 + (i % 4) * 0.5  # Longer response times\n",
    "    } for i in range(30)]\n",
    "]\n",
    "\n",
    "# Initialize our custom analyzer with sample data\n",
    "customer_analyzer = CustomerServicePerformanceAnalyzer(interaction_data)\n",
    "\n",
    "# Display the specialized metrics\n",
    "prompt_analysis = customer_analyzer.analyze_prompt_success()\n",
    "satisfaction_metrics = customer_analyzer.get_customer_satisfaction_metrics()\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "df = pd.DataFrame({\n",
    "    template_id: {\n",
    "        \"success_rate\": metrics[\"success_rate\"],\n",
    "        \"error_rate\": metrics[\"error_rate\"],\n",
    "        \"resolution_rate\": metrics[\"resolution_rate\"],\n",
    "        \"handoff_rate\": metrics[\"handoff_rate\"],\n",
    "        \"avg_satisfaction\": metrics[\"average_satisfaction\"],\n",
    "        \"avg_response_time\": metrics[\"average_response_time\"]\n",
    "    }\n",
    "    for template_id, metrics in prompt_analysis.items()\n",
    "}).T\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Customer Service Performance Metrics:\")\n",
    "display(df)\n",
    "\n",
    "# Display satisfaction distribution\n",
    "print(\"\\nCustomer Satisfaction Distribution:\")\n",
    "for category, count in satisfaction_metrics[\"distribution\"].items():\n",
    "    print(f\"- {category}: {count} interactions\")\n",
    "\n",
    "# Visualize key metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot success rates\n",
    "df[[\"success_rate\", \"resolution_rate\"]].plot(kind=\"bar\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Success & Resolution Rates by Template\")\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Plot error and handoff rates\n",
    "df[[\"error_rate\", \"handoff_rate\"]].plot(kind=\"bar\", ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Error & Handoff Rates by Template\")\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# Plot average satisfaction\n",
    "df[\"avg_satisfaction\"].plot(kind=\"bar\", ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Average Satisfaction by Template\")\n",
    "axes[1, 0].set_ylim(0, 5)  # Assuming 5-star scale\n",
    "\n",
    "# Plot average response time\n",
    "df[\"avg_response_time\"].plot(kind=\"bar\", ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Average Response Time by Template (seconds)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25026014",
   "metadata": {},
   "source": [
    "## 2. Creating a Custom AdaptiveLearningManager\n",
    "\n",
    "Now let's create a specialized AdaptiveLearningManager for customer service templates that has domain-specific optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6433b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerServiceLearningManager(AdaptiveLearningManager):\n",
    "    \"\"\"\n",
    "    A specialized AdaptiveLearningManager for customer service chatbots with:\n",
    "    - Domain-specific optimization strategies\n",
    "    - Customer satisfaction-focused improvements\n",
    "    - Response time optimization\n",
    "    - Handoff reduction strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, performance_analyzer, prompt_library=None, config=None, ai_engine=None):\n",
    "        \"\"\"Initialize with customer service specific configuration defaults.\"\"\"\n",
    "        # Set default config values for customer service scenarios\n",
    "        default_config = {\n",
    "            \"error_rate_threshold\": 0.2,  # Lower threshold for customer-facing services\n",
    "            \"feedback_optimization_threshold\": 0.6,  # Higher bar for customer satisfaction\n",
    "            \"resolution_rate_threshold\": 0.7,  # Expect 70%+ resolution rate\n",
    "            \"handoff_rate_threshold\": 0.2,  # Keep handoffs below 20%\n",
    "            \"response_time_threshold\": 2.0,  # Target response time below 2 seconds\n",
    "            \"auto_optimize_prompts\": True\n",
    "        }\n",
    "        \n",
    "        # Override defaults with provided config\n",
    "        if config:\n",
    "            default_config.update(config)\n",
    "            \n",
    "        # Call parent initializer with updated config\n",
    "        super().__init__(\n",
    "            performance_analyzer=performance_analyzer,\n",
    "            prompt_library=prompt_library,\n",
    "            config=default_config,\n",
    "            ai_engine=ai_engine\n",
    "        )\n",
    "    \n",
    "    async def _generate_rule_based_improvement(self, template, metrics, suggestion):\n",
    "        \"\"\"\n",
    "        Enhanced rule-based improvement with customer service specific strategies.\n",
    "        \"\"\"\n",
    "        prompt = template.user_prompt_template\n",
    "        improvements_applied = []\n",
    "        \n",
    "        # Strategy 1: Improve clarity for high error rates\n",
    "        if metrics.get(\"error_rate\", 0) > self.config.get(\"error_rate_threshold\", 0.2):\n",
    "            original_prompt = prompt\n",
    "            prompt = prompt.replace(\"provide\", \"provide step-by-step\")\n",
    "            prompt = prompt.replace(\"tell me\", \"explain clearly\")\n",
    "            if prompt != original_prompt:\n",
    "                improvements_applied.append(\"Improved clarity for error reduction\")\n",
    "            \n",
    "        # Strategy 2: Add empathy for low satisfaction scores\n",
    "        if metrics.get(\"average_satisfaction\", 5) < self.config.get(\"feedback_optimization_threshold\", 4):\n",
    "            if not any(word in prompt.lower() for word in [\"sorry\", \"understand\", \"appreciate\"]):\n",
    "                if \"?\" in prompt:\n",
    "                    prompt = prompt.replace(\"?\", \"? I understand this might be frustrating, \")\n",
    "                else:\n",
    "                    prompt += \" I'm here to help you with this matter.\"\n",
    "                improvements_applied.append(\"Added empathy for improved satisfaction\")\n",
    "        \n",
    "        # Strategy 3: Add specificity for high handoff rates\n",
    "        if metrics.get(\"handoff_rate\", 0) > self.config.get(\"handoff_rate_threshold\", 0.2):\n",
    "            if not \"specific\" in prompt.lower():\n",
    "                prompt = prompt.replace(\".\", \". Please provide specific details so I can assist you better.\")\n",
    "                improvements_applied.append(\"Added request for specific details to reduce handoffs\")\n",
    "        \n",
    "        # Strategy 4: Streamline for slow response times\n",
    "        if metrics.get(\"average_response_time\", 0) > self.config.get(\"response_time_threshold\", 2.0):\n",
    "            # Find and remove verbose phrases\n",
    "            verbose_phrases = [\"if you don't mind\", \"would you be so kind\", \"if it's not too much trouble\"]\n",
    "            for phrase in verbose_phrases:\n",
    "                prompt = prompt.replace(phrase, \"\")\n",
    "            if not any(phrase in prompt for phrase in verbose_phrases):\n",
    "                # Already concise, just note it\n",
    "                improvements_applied.append(\"Maintained concise phrasing for response time\")\n",
    "            else:\n",
    "                improvements_applied.append(\"Streamlined wording for faster response time\")\n",
    "                \n",
    "        # If no improvements were made, return the original\n",
    "        if not improvements_applied:\n",
    "            return template.user_prompt_template\n",
    "            \n",
    "        # Return the improved prompt with optimization notes\n",
    "        return prompt\n",
    "    \n",
    "    async def suggest_customer_service_improvements(self, underperforming_prompts):\n",
    "        \"\"\"\n",
    "        Suggest customer service specific improvements based on performance data.\n",
    "        \"\"\"\n",
    "        if not underperforming_prompts:\n",
    "            return {}\n",
    "            \n",
    "        suggestions = {}\n",
    "        for prompt_id, data in underperforming_prompts.items():\n",
    "            if data.get(\"handoff_rate\", 0) > self.config.get(\"handoff_rate_threshold\", 0.2):\n",
    "                suggestions[prompt_id] = {\n",
    "                    \"issue\": \"High handoff rate\",\n",
    "                    \"suggestion\": \"Add more specific information gathering questions and provide more detailed responses.\"\n",
    "                }\n",
    "            elif data.get(\"average_satisfaction\", 5) < self.config.get(\"feedback_optimization_threshold\", 3.5):\n",
    "                suggestions[prompt_id] = {\n",
    "                    \"issue\": \"Low customer satisfaction\",\n",
    "                    \"suggestion\": \"Add empathetic language and acknowledgment of customer concerns.\"\n",
    "                }\n",
    "            elif data.get(\"error_rate\", 0) > self.config.get(\"error_rate_threshold\", 0.2):\n",
    "                suggestions[prompt_id] = {\n",
    "                    \"issue\": \"High error rate\",\n",
    "                    \"suggestion\": \"Simplify instructions and add step-by-step guidance.\"\n",
    "                }\n",
    "                \n",
    "        return suggestions\n",
    "    \n",
    "    async def analyze_customer_service_metrics(self):\n",
    "        \"\"\"\n",
    "        Generate specialized customer service insights from the performance data.\n",
    "        \"\"\"\n",
    "        prompt_analysis = self.performance_analyzer.analyze_prompt_success()\n",
    "        \n",
    "        if not hasattr(self.performance_analyzer, \"get_customer_satisfaction_metrics\"):\n",
    "            return {\"error\": \"Performance analyzer doesn't support customer satisfaction metrics\"}\n",
    "            \n",
    "        satisfaction_metrics = self.performance_analyzer.get_customer_satisfaction_metrics()\n",
    "        \n",
    "        # Find templates with concerning metrics\n",
    "        concerning_templates = []\n",
    "        for template_id, metrics in prompt_analysis.items():\n",
    "            issues = []\n",
    "            \n",
    "            if metrics.get(\"handoff_rate\", 0) > self.config.get(\"handoff_rate_threshold\", 0.2):\n",
    "                issues.append(f\"High handoff rate: {metrics['handoff_rate']:.1%}\")\n",
    "                \n",
    "            if metrics.get(\"average_satisfaction\", 5) < self.config.get(\"feedback_optimization_threshold\", 3.5):\n",
    "                issues.append(f\"Low satisfaction: {metrics['average_satisfaction']:.1f}/5.0\")\n",
    "                \n",
    "            if metrics.get(\"resolution_rate\", 1) < self.config.get(\"resolution_rate_threshold\", 0.7):\n",
    "                issues.append(f\"Low resolution rate: {metrics['resolution_rate']:.1%}\")\n",
    "                \n",
    "            if issues:\n",
    "                concerning_templates.append({\n",
    "                    \"template_id\": template_id,\n",
    "                    \"issues\": issues,\n",
    "                    \"metrics\": {k: metrics[k] for k in [\"handoff_rate\", \"average_satisfaction\", \"resolution_rate\", \"error_rate\", \"success_rate\"]}\n",
    "                })\n",
    "                \n",
    "        return {\n",
    "            \"overall_satisfaction\": satisfaction_metrics.get(\"overall_satisfaction\"),\n",
    "            \"concerning_templates\": concerning_templates,\n",
    "            \"optimization_opportunities\": len(concerning_templates)\n",
    "        }\n",
    "\n",
    "# Initialize our specialized learning manager\n",
    "customer_manager = CustomerServiceLearningManager(\n",
    "    performance_analyzer=customer_analyzer,\n",
    "    config={\n",
    "        \"handoff_rate_threshold\": 0.25,  # Allow up to 25% handoffs\n",
    "        \"response_time_threshold\": 1.5    # Target 1.5 second response time\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test the specialized analyzers and optimizers\n",
    "service_insights = await customer_manager.analyze_customer_service_metrics()\n",
    "\n",
    "# Display the customer service specific insights\n",
    "print(\"Customer Service Insights:\")\n",
    "print(f\"- Overall satisfaction: {service_insights['overall_satisfaction']:.2f}/5.0\")\n",
    "print(f\"- Templates needing optimization: {len(service_insights['concerning_templates'])}\")\n",
    "\n",
    "for i, template in enumerate(service_insights['concerning_templates']):\n",
    "    print(f\"\\n{i+1}. Template: {template['template_id']}\")\n",
    "    print(f\"   Issues: {', '.join(template['issues'])}\")\n",
    "    metrics = template['metrics']\n",
    "    for metric, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcee3fe",
   "metadata": {},
   "source": [
    "## 3. Creating a Specialized PromptLibrary \n",
    "\n",
    "Now let's create a PromptLibrary extension for customer service prompts with domain-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory for our prompt templates\n",
    "import tempfile\n",
    "cs_prompt_library_path = tempfile.mkdtemp()\n",
    "print(f\"Created temporary directory for templates: {cs_prompt_library_path}\")\n",
    "\n",
    "class CustomerServicePromptLibrary(PromptLibrary):\n",
    "    \"\"\"\n",
    "    Enhanced PromptLibrary with customer service specific features:\n",
    "    - Special template categories for different service scenarios\n",
    "    - Template recommendations based on customer context\n",
    "    - A/B testing capabilities for measuring customer satisfaction impact\n",
    "    - Enhanced versioning with roll-back capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize the customer service prompt library.\"\"\"\n",
    "        super().__init__(config)\n",
    "        self._categories = {}  # Map of category -> template_ids\n",
    "        self._categorize_templates()\n",
    "    \n",
    "    def _categorize_templates(self):\n",
    "        \"\"\"Categorize loaded templates based on their tags.\"\"\"\n",
    "        self._categories = {}\n",
    "        for template_id, template in self._templates.items():\n",
    "            for tag in template.tags:\n",
    "                if tag not in self._categories:\n",
    "                    self._categories[tag] = []\n",
    "                self._categories[tag].append(template_id)\n",
    "    \n",
    "    def get_templates_by_category(self, category):\n",
    "        \"\"\"Get all templates in a specific category.\"\"\"\n",
    "        template_ids = self._categories.get(category, [])\n",
    "        return [self.get_template(tid) for tid in template_ids]\n",
    "    \n",
    "    def list_categories(self):\n",
    "        \"\"\"List all available categories.\"\"\"\n",
    "        return list(self._categories.keys())\n",
    "    \n",
    "    def recommend_template(self, customer_context):\n",
    "        \"\"\"\n",
    "        Recommend a template based on customer context.\n",
    "        \n",
    "        :param customer_context: Dict with details like issue_type, \n",
    "                                customer_history, sentiment, etc.\n",
    "        :return: Best matching template or None\n",
    "        \"\"\"\n",
    "        issue_type = customer_context.get(\"issue_type\")\n",
    "        if issue_type and issue_type in self._categories:\n",
    "            # Find templates matching the issue type\n",
    "            candidates = self._categories[issue_type]\n",
    "            if candidates:\n",
    "                # Simple recommendation - just return the first match\n",
    "                # In real implementation, this would use more sophisticated matching\n",
    "                return self.get_template(candidates[0])\n",
    "        \n",
    "        # Fallback to default if available\n",
    "        if self.config.default_prompt_id:\n",
    "            return self.get_template(self.config.default_prompt_id)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def rollback_template(self, template_id, to_version):\n",
    "        \"\"\"\n",
    "        Roll back a template to a previous version.\n",
    "        \n",
    "        :param template_id: The ID of the template\n",
    "        :param to_version: The version to roll back to\n",
    "        :return: The rolled-back template or None if not possible\n",
    "        \"\"\"\n",
    "        # Find the target version\n",
    "        target = None\n",
    "        if template_id in self._version_history:\n",
    "            for version in self._version_history[template_id]:\n",
    "                if version.version == to_version:\n",
    "                    target = version\n",
    "                    break\n",
    "        \n",
    "        # If not in history, check if it's the current version\n",
    "        if not target and template_id in self._templates:\n",
    "            current = self._templates[template_id]\n",
    "            if current.version == to_version:\n",
    "                target = current\n",
    "        \n",
    "        if not target:\n",
    "            return None\n",
    "        \n",
    "        # Create a new version based on the target\n",
    "        rollback = copy.deepcopy(target)\n",
    "        \n",
    "        # Increment version and update metadata\n",
    "        current_version = self._templates[template_id].version if template_id in self._templates else 0\n",
    "        rollback.version = current_version + 1\n",
    "        rollback.updated_at = time.time()\n",
    "        rollback.updated_by_component = \"rollback\"\n",
    "        rollback.version_notes = f\"Rollback to version {to_version}\"\n",
    "        \n",
    "        # Add the rollback version\n",
    "        self.add_template(rollback, overwrite=True)\n",
    "        \n",
    "        return rollback\n",
    "\n",
    "# Create a specialized config for customer service prompts\n",
    "cs_config = PromptLibraryConfig(\n",
    "    library_path=cs_prompt_library_path,\n",
    "    default_prompt_id=\"greeting\",\n",
    "    auto_save=True\n",
    ")\n",
    "\n",
    "# Define sample customer service templates\n",
    "cs_templates = [\n",
    "    {\n",
    "        \"template_id\": \"greeting\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"Customer greeting template\",\n",
    "        \"system_prompt\": \"You are a helpful customer service representative.\",\n",
    "        \"user_prompt_template\": \"Welcome to our customer service. How can I assist you today?\",\n",
    "        \"placeholders\": [],\n",
    "        \"tags\": [\"greeting\", \"general\"],\n",
    "        \"created_at\": time.time()\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": \"order_status\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"Template for checking order status\",\n",
    "        \"system_prompt\": \"You are a helpful order management assistant.\",\n",
    "        \"user_prompt_template\": \"I'll help you check the status of your order. Can you please provide your order number for {order_type}?\",\n",
    "        \"placeholders\": [\"order_type\"],\n",
    "        \"tags\": [\"orders\", \"status\"],\n",
    "        \"created_at\": time.time()\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": \"return_policy\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"Template for explaining return policies\",\n",
    "        \"system_prompt\": \"You are a helpful returns department assistant.\",\n",
    "        \"user_prompt_template\": \"Here's our return policy for {product_category} items: Returns are accepted within {days} days of purchase.\",\n",
    "        \"placeholders\": [\"product_category\", \"days\"],\n",
    "        \"tags\": [\"returns\", \"policy\"],\n",
    "        \"created_at\": time.time()\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save templates to JSON files\n",
    "for template_data in cs_templates:\n",
    "    filename = f\"{template_data['template_id']}_v{template_data['version']}.json\"\n",
    "    filepath = os.path.join(cs_prompt_library_path, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(template_data, f, indent=2)\n",
    "\n",
    "# Initialize the specialized prompt library\n",
    "cs_prompt_library = CustomerServicePromptLibrary(cs_config)\n",
    "\n",
    "# Test the specialized features\n",
    "print(\"Available categories:\")\n",
    "categories = cs_prompt_library.list_categories()\n",
    "for category in categories:\n",
    "    print(f\"- {category}: {len(cs_prompt_library.get_templates_by_category(category))} templates\")\n",
    "\n",
    "# Test template recommendation\n",
    "customer_context = {\"issue_type\": \"returns\", \"sentiment\": \"frustrated\"}\n",
    "recommended = cs_prompt_library.recommend_template(customer_context)\n",
    "if recommended:\n",
    "    print(f\"\\nRecommended template for returns issue: {recommended.template_id} (v{recommended.version})\")\n",
    "    print(f\"- Description: {recommended.description}\")\n",
    "    print(f\"- Template: {recommended.user_prompt_template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2bc78",
   "metadata": {},
   "source": [
    "## 4. Creating a Complete End-to-End Optimization Workflow\n",
    "\n",
    "Now let's put it all together into a complete end-to-end workflow using our custom components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_customer_service_optimization_workflow():\n",
    "    \"\"\"\n",
    "    Demonstrate a complete end-to-end workflow for customer service prompt optimization.\n",
    "    \"\"\"\n",
    "    print(\"Starting customer service optimization workflow...\")\n",
    "    \n",
    "    # Step 1: Setup and initialization (already done above)\n",
    "    library = cs_prompt_library\n",
    "    analyzer = customer_analyzer\n",
    "    manager = customer_manager\n",
    "    \n",
    "    # Connect the manager to the library\n",
    "    manager.prompt_library = library\n",
    "    \n",
    "    # Step 2: Record initial state of templates\n",
    "    print(\"\\nInitial template states:\")\n",
    "    initial_templates = {}\n",
    "    for template_id in library.list_template_ids():\n",
    "        template = library.get_template(template_id)\n",
    "        initial_templates[template_id] = template\n",
    "        print(f\"- {template_id} (v{template.version}): {template.user_prompt_template}\")\n",
    "    \n",
    "    # Step 3: Analyze customer service metrics\n",
    "    print(\"\\nAnalyzing customer service metrics...\")\n",
    "    cs_metrics = await manager.analyze_customer_service_metrics()\n",
    "    \n",
    "    # Step 4: Identify underperforming templates\n",
    "    underperforming = manager.identify_underperforming_prompts({\n",
    "        \"success_rate_threshold\": 0.7,\n",
    "        \"min_sample_size\": 10\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nIdentified {len(underperforming)} underperforming templates:\")\n",
    "    for template_id, data in underperforming.items():\n",
    "        print(f\"- {template_id}: Success rate {data.get('success_rate', 0):.2f}, Error rate {data.get('error_rate', 0):.2f}\")\n",
    "    \n",
    "    # Step 5: Generate improvement suggestions\n",
    "    print(\"\\nGenerating customer service specific suggestions...\")\n",
    "    suggestions = await manager.suggest_customer_service_improvements(underperforming)\n",
    "    \n",
    "    print(\"Improvement suggestions:\")\n",
    "    for template_id, suggestion in suggestions.items():\n",
    "        print(f\"- {template_id}: {suggestion['issue']}\")\n",
    "        print(f\"  Suggestion: {suggestion['suggestion']}\")\n",
    "    \n",
    "    # Step 6: Run the optimization cycle with auto-apply\n",
    "    print(\"\\nRunning optimization cycle with auto-apply=True...\")\n",
    "    cycle_results = await manager.run_learning_cycle(auto_optimize=True)\n",
    "    \n",
    "    # Step 7: Check results and report changes\n",
    "    print(\"\\nOptimization cycle completed.\")\n",
    "    optimized_count = len(cycle_results.get('optimized_prompts', []))\n",
    "    print(f\"Optimized {optimized_count} templates\")\n",
    "    \n",
    "    # Step 8: Show before/after comparison\n",
    "    print(\"\\nTemplate changes:\")\n",
    "    for template_id in library.list_template_ids():\n",
    "        template = library.get_template(template_id)\n",
    "        initial = initial_templates.get(template_id)\n",
    "        \n",
    "        if template.version > initial.version:\n",
    "            print(f\"\\n{template_id} (v{initial.version} â†’ v{template.version}):\")\n",
    "            print(f\"BEFORE: {initial.user_prompt_template}\")\n",
    "            print(f\"AFTER:  {template.user_prompt_template}\")\n",
    "            \n",
    "            if hasattr(template, 'version_notes') and template.version_notes:\n",
    "                print(f\"NOTES:  {template.version_notes}\")\n",
    "    \n",
    "    # Step 9: Create visualization of improvements\n",
    "    if optimized_count > 0:\n",
    "        # Collect metrics for visualization\n",
    "        print(\"\\nVisualizing changes in key metrics:\")\n",
    "        \n",
    "        # This would normally compare before/after metrics\n",
    "        # For this example, we'll just show the current metrics\n",
    "        prompt_analysis = analyzer.analyze_prompt_success()\n",
    "        \n",
    "        # Extract key metrics for optimized templates\n",
    "        metrics_df = pd.DataFrame({\n",
    "            template_id: {\n",
    "                \"Handoff Rate\": metrics.get(\"handoff_rate\", 0),\n",
    "                \"Resolution Rate\": metrics.get(\"resolution_rate\", 0),\n",
    "                \"Satisfaction\": metrics.get(\"average_satisfaction\", 0),\n",
    "                \"Error Rate\": metrics.get(\"error_rate\", 0)\n",
    "            }\n",
    "            for template_id, metrics in prompt_analysis.items()\n",
    "            if template_id in [t[\"template_id\"] for t in cs_metrics.get(\"concerning_templates\", [])]\n",
    "        }).T\n",
    "        \n",
    "        # Display metrics\n",
    "        if not metrics_df.empty:\n",
    "            display(metrics_df)\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            metrics_df.plot(kind=\"bar\", ax=ax)\n",
    "            ax.set_title(\"Key Metrics for Optimized Templates\")\n",
    "            ax.set_ylim(0, 5)  # Assuming 0-5 scale for satisfaction, 0-1 for rates\n",
    "            \n",
    "            # Add threshold lines\n",
    "            ax.axhline(y=manager.config[\"handoff_rate_threshold\"], color='r', linestyle='--', \n",
    "                      label=f\"Handoff Threshold ({manager.config['handoff_rate_threshold']})\")\n",
    "            ax.axhline(y=manager.config[\"resolution_rate_threshold\"], color='g', linestyle='--',\n",
    "                      label=f\"Resolution Threshold ({manager.config['resolution_rate_threshold']})\")\n",
    "            \n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return {\n",
    "        \"templates_analyzed\": len(library.list_template_ids()),\n",
    "        \"templates_optimized\": optimized_count,\n",
    "        \"optimization_cycle_id\": cycle_results.get(\"cycle_id\")\n",
    "    }\n",
    "\n",
    "# Run the complete workflow\n",
    "workflow_results = await run_customer_service_optimization_workflow()\n",
    "print(f\"\\nWorkflow completed: {workflow_results['templates_optimized']}/{workflow_results['templates_analyzed']} templates optimized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff489d",
   "metadata": {},
   "source": [
    "## Summary: Building Your Own Specialized Workflow\n",
    "\n",
    "In this notebook, we've demonstrated how to extend the base prompt optimization framework with custom implementations for a specific domain (customer service). The key components we've built include:\n",
    "\n",
    "1. **Specialized PerformanceAnalyzer**: Added domain-specific metrics like resolution rate, handoff rate, and customer satisfaction.\n",
    "\n",
    "2. **Enhanced AdaptiveLearningManager**: Implemented customer service-specific optimization strategies focused on empathy, clarity, and response times.\n",
    "\n",
    "3. **Extended PromptLibrary**: Added features like categorization, template recommendations, and rollback capabilities.\n",
    "\n",
    "4. **Complete End-to-End Workflow**: Tied everything together into a cohesive process for analyzing and optimizing customer service prompts.\n",
    "\n",
    "This pattern can be applied to any domain by:\n",
    "- Identifying the domain-specific metrics that matter\n",
    "- Creating specialized optimization strategies for those metrics\n",
    "- Building domain-appropriate extensions to the core components\n",
    "- Creating a workflow that ties them all together\n",
    "\n",
    "The modular design of the framework makes it flexible enough to adapt to many different use cases while maintaining a consistent core functionality."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
