{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee554c81",
   "metadata": {},
   "source": [
    "# Prompt Optimization with Real LLM APIs\n",
    "\n",
    "This notebook demonstrates how to integrate the prompt optimization workflow with real LLM APIs, including:\n",
    "\n",
    "1. Connecting to OpenAI and Anthropic APIs\n",
    "2. Creating real performance measurements for prompt templates\n",
    "3. Running optimization cycles with real LLM responses\n",
    "4. Analyzing cost/performance tradeoffs in prompt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5e47dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import AILF framework components\n",
    "from ailf.cognition.prompt_library import PromptLibrary\n",
    "from ailf.schemas.prompt_engineering import PromptTemplateV1, PromptLibraryConfig\n",
    "from ailf.feedback.performance_analyzer import PerformanceAnalyzer\n",
    "from ailf.feedback.adaptive_learning_manager import AdaptiveLearningManager\n",
    "from ailf.ai.engine import AIEngine\n",
    "\n",
    "# Load environment variables for API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API keys are available\n",
    "api_keys_available = {\n",
    "    \"OPENAI_API_KEY\": bool(os.getenv(\"OPENAI_API_KEY\")),\n",
    "    \"ANTHROPIC_API_KEY\": bool(os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "}\n",
    "\n",
    "print(\"API Keys Available:\")\n",
    "for api, available in api_keys_available.items():\n",
    "    print(f\"- {api}: {'✓' if available else '✗'}\")\n",
    "\n",
    "# If no API keys are available, provide instructions\n",
    "if not any(api_keys_available.values()):\n",
    "    print(\"\\nNo API keys found. To run this notebook with real APIs:\")\n",
    "    print(\"1. Create a .env file in the project root\")\n",
    "    print(\"2. Add your API keys in the following format:\")\n",
    "    print(\"   OPENAI_API_KEY=your_key_here\")\n",
    "    print(\"   ANTHROPIC_API_KEY=your_key_here\")\n",
    "    print(\"3. Restart the kernel and run the notebook again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5956243",
   "metadata": {},
   "source": [
    "## 1. Setting up the AI Engine with Real API Providers\n",
    "\n",
    "First, let's set up the AI Engine that will connect to the LLM APIs. We'll support both OpenAI and Anthropic, with graceful fallback if one isn't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configured AI Engine\n",
    "async def create_ai_engine():\n",
    "    \"\"\"Create and initialize an AI Engine with available providers.\"\"\"\n",
    "    # Default configuration\n",
    "    config = {\n",
    "        \"default_provider\": None,\n",
    "        \"log_prompts\": True,\n",
    "        \"log_responses\": True,\n",
    "        \"providers\": {}\n",
    "    }\n",
    "    \n",
    "    # Configure OpenAI if available\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if openai_api_key:\n",
    "        config[\"providers\"][\"openai\"] = {\n",
    "            \"api_key\": openai_api_key,\n",
    "            \"default_model\": \"gpt-4o-mini\",\n",
    "            \"default_temperature\": 0.2,\n",
    "            \"timeout\": 30,\n",
    "            \"enabled\": True\n",
    "        }\n",
    "        if not config[\"default_provider\"]:\n",
    "            config[\"default_provider\"] = \"openai\"\n",
    "    \n",
    "    # Configure Anthropic if available\n",
    "    anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    if anthropic_api_key:\n",
    "        config[\"providers\"][\"anthropic\"] = {\n",
    "            \"api_key\": anthropic_api_key,\n",
    "            \"default_model\": \"claude-3-haiku-20240307\",\n",
    "            \"default_temperature\": 0.2,\n",
    "            \"timeout\": 30,\n",
    "            \"enabled\": True\n",
    "        }\n",
    "        if not config[\"default_provider\"]:\n",
    "            config[\"default_provider\"] = \"anthropic\"\n",
    "    \n",
    "    # Create engine\n",
    "    engine = AIEngine(config)\n",
    "    await engine.initialize()\n",
    "    \n",
    "    return engine\n",
    "\n",
    "# Initialize the engine if we have at least one API key\n",
    "ai_engine = None\n",
    "if any(api_keys_available.values()):\n",
    "    ai_engine = await create_ai_engine()\n",
    "    print(f\"\\nAI Engine initialized with {ai_engine.config['default_provider']} as default provider.\")\n",
    "    print(f\"Available models:\")\n",
    "    for provider, config in ai_engine.config[\"providers\"].items():\n",
    "        if config[\"enabled\"]:\n",
    "            print(f\"- {provider}: {config['default_model']}\")\n",
    "else:\n",
    "    print(\"\\nNo API keys available. Running in demo mode with simulated responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b105417",
   "metadata": {},
   "source": [
    "## 2. Creating a Prompt Library for Real-World Testing\n",
    "\n",
    "Now, let's create a PromptLibrary with templates for real-world tasks that we'll optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84caedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory for our prompt templates\n",
    "import tempfile\n",
    "import copy\n",
    "library_path = tempfile.mkdtemp()\n",
    "print(f\"Created temporary directory for templates: {library_path}\")\n",
    "\n",
    "# Create a configuration for our PromptLibrary\n",
    "config = PromptLibraryConfig(\n",
    "    library_path=library_path,\n",
    "    default_prompt_id=\"sentiment_analysis\",\n",
    "    auto_save=True\n",
    ")\n",
    "\n",
    "# Define real-world prompt templates for testing\n",
    "templates = [\n",
    "    {\n",
    "        \"template_id\": \"sentiment_analysis\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"Analyze sentiment of text (positive, negative, neutral)\",\n",
    "        \"system_prompt\": \"You are an assistant that analyzes the sentiment of text.\",\n",
    "        \"user_prompt_template\": \"Determine if the sentiment of this text is positive, negative, or neutral.\\n\\nText: {input_text}\\n\\nSentiment:\",\n",
    "        \"placeholders\": [\"input_text\"],\n",
    "        \"tags\": [\"sentiment_analysis\", \"classification\"],\n",
    "        \"created_at\": time.time()\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": \"question_answering\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"Answer questions based on provided context\",\n",
    "        \"system_prompt\": \"You are a helpful assistant that answers questions based on the provided context.\",\n",
    "        \"user_prompt_template\": \"Context: {context}\\n\\nQuestion: {question}\\n\\nPlease provide a concise answer based only on the context provided.\",\n",
    "        \"placeholders\": [\"context\", \"question\"],\n",
    "        \"tags\": [\"qa\", \"information_retrieval\"],\n",
    "        \"created_at\": time.time()\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": \"code_generation\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"Generate code based on requirements\",\n",
    "        \"system_prompt\": \"You are a helpful coding assistant.\",\n",
    "        \"user_prompt_template\": \"Write code in {language} that accomplishes the following:\\n\\n{requirements}\\n\\nProvide only the code without explanations.\",\n",
    "        \"placeholders\": [\"language\", \"requirements\"],\n",
    "        \"tags\": [\"code\", \"generation\"],\n",
    "        \"created_at\": time.time()\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save templates to JSON files\n",
    "for template_data in templates:\n",
    "    filename = f\"{template_data['template_id']}_v{template_data['version']}.json\"\n",
    "    filepath = os.path.join(library_path, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(template_data, f, indent=2)\n",
    "\n",
    "# Initialize our PromptLibrary\n",
    "prompt_library = PromptLibrary(config)\n",
    "\n",
    "# List available templates\n",
    "print(\"\\nAvailable templates for testing:\")\n",
    "for template_id in prompt_library.list_template_ids():\n",
    "    template = prompt_library.get_template(template_id)\n",
    "    print(f\"- {template_id} (v{template.version}): {template.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6eee5",
   "metadata": {},
   "source": [
    "## 3. Creating Test Data for Real-World Evaluation\n",
    "\n",
    "Let's create some test data for each template to evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data for each template\n",
    "test_data = {\n",
    "    \"sentiment_analysis\": [\n",
    "        {\"input_text\": \"I absolutely loved the product! It exceeded all my expectations.\", \"expected\": \"positive\"},\n",
    "        {\"input_text\": \"The service was okay, nothing special but it got the job done.\", \"expected\": \"neutral\"},\n",
    "        {\"input_text\": \"This was a complete waste of money. I'm very disappointed.\", \"expected\": \"negative\"},\n",
    "        {\"input_text\": \"While there were some issues with delivery, the product quality is amazing.\", \"expected\": \"mixed\"},\n",
    "        {\"input_text\": \"I can't believe how terrible their customer service is. Never shopping here again.\", \"expected\": \"negative\"},\n",
    "        {\"input_text\": \"The restaurant was fine, food was average, prices were reasonable.\", \"expected\": \"neutral\"},\n",
    "        {\"input_text\": \"Best purchase I've made all year! Highly recommend to everyone.\", \"expected\": \"positive\"},\n",
    "        {\"input_text\": \"It's an interesting concept but the execution leaves much to be desired.\", \"expected\": \"mixed\"}\n",
    "    ],\n",
    "    \n",
    "    \"question_answering\": [\n",
    "        {\n",
    "            \"context\": \"The first Olympic Games were held in 776 BC in Olympia, Greece. They were held every four years in honor of Zeus, the king of the Greek gods. The modern Olympic Games began in 1896 in Athens, Greece.\",\n",
    "            \"question\": \"When did the modern Olympic Games begin?\",\n",
    "            \"expected\": \"1896\"\n",
    "        },\n",
    "        {\n",
    "            \"context\": \"The Great Barrier Reef is the world's largest coral reef system, stretching for over 2,300 kilometers along the northeast coast of Australia. It consists of over 2,900 individual reefs and 900 islands.\",\n",
    "            \"question\": \"Where is the Great Barrier Reef located?\",\n",
    "            \"expected\": \"Australia\"\n",
    "        },\n",
    "        {\n",
    "            \"context\": \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two different scientific fields, and the first woman to become a professor at the University of Paris.\",\n",
    "            \"question\": \"How many Nobel Prizes did Marie Curie win?\",\n",
    "            \"expected\": \"two\"\n",
    "        },\n",
    "        {\n",
    "            \"context\": \"The python programming language was created by Guido van Rossum and first released in 1991. It emphasizes code readability with its notable use of significant whitespace.\",\n",
    "            \"question\": \"Who created the Python programming language?\",\n",
    "            \"expected\": \"Guido van Rossum\"\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"code_generation\": [\n",
    "        {\n",
    "            \"language\": \"Python\",\n",
    "            \"requirements\": \"Create a function to calculate the Fibonacci sequence up to n terms\",\n",
    "            \"expected_contains\": [\"def\", \"fibonacci\", \"return\"]\n",
    "        },\n",
    "        {\n",
    "            \"language\": \"JavaScript\",\n",
    "            \"requirements\": \"Create a function that filters an array to only include even numbers\",\n",
    "            \"expected_contains\": [\"function\", \"filter\", \"even\", \"return\"]\n",
    "        },\n",
    "        {\n",
    "            \"language\": \"Python\",\n",
    "            \"requirements\": \"Write a function that checks if a string is a palindrome\",\n",
    "            \"expected_contains\": [\"def\", \"palindrome\", \"return\"]\n",
    "        },\n",
    "        {\n",
    "            \"language\": \"SQL\",\n",
    "            \"requirements\": \"Write a query to find the top 5 customers by total purchase amount\",\n",
    "            \"expected_contains\": [\"SELECT\", \"FROM\", \"ORDER BY\", \"LIMIT\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Show the test data for each template\n",
    "for template_id, dataset in test_data.items():\n",
    "    print(f\"\\nTest Data for {template_id} ({len(dataset)} samples):\")\n",
    "    \n",
    "    for i, example in enumerate(dataset[:2]):  # Only show first 2 examples\n",
    "        print(f\"- Example {i+1}:\")\n",
    "        for key, value in example.items():\n",
    "            if key == \"context\" and len(value) > 100:\n",
    "                print(f\"  {key}: {value[:100]}...\")  # Truncate long context\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    if len(dataset) > 2:\n",
    "        print(f\"  ... and {len(dataset) - 2} more examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbfcd5",
   "metadata": {},
   "source": [
    "## 4. Testing Prompt Templates with Real LLM APIs\n",
    "\n",
    "Now, let's evaluate our prompt templates using the real LLM APIs and collect performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fae364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to test templates with real LLM APIs\n",
    "async def test_template(engine, template_id, test_cases, performance_analyzer):\n",
    "    \"\"\"Test a template with real LLM responses and record performance data.\"\"\"\n",
    "    template = prompt_library.get_template(template_id)\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing template: {template_id}\")\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"  Running test case {i+1}/{len(test_cases)}...\", end=\"\")\n",
    "        \n",
    "        # Fill in the template with test data\n",
    "        filled_template = template.user_prompt_template\n",
    "        for placeholder in template.placeholders:\n",
    "            if placeholder in test_case:\n",
    "                filled_template = filled_template.replace(f\"{{{placeholder}}}\", str(test_case[placeholder]))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        error = None\n",
    "        \n",
    "        try:\n",
    "            # Make the actual API call\n",
    "            response = await engine.generate(\n",
    "                system=template.system_prompt,\n",
    "                user=filled_template,\n",
    "                provider=engine.config[\"default_provider\"]\n",
    "            )\n",
    "            \n",
    "            result = response.get(\"content\", \"\")\n",
    "            \n",
    "            # Evaluate success based on expected output\n",
    "            successful = False\n",
    "            if \"expected\" in test_case:\n",
    "                expected = test_case[\"expected\"].lower()\n",
    "                successful = expected in result.lower()\n",
    "            elif \"expected_contains\" in test_case:\n",
    "                successful = all(item.lower() in result.lower() for item in test_case[\"expected_contains\"])\n",
    "            \n",
    "            print(f\" {'✓' if successful else '✗'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "            result = None\n",
    "            successful = False\n",
    "            print(f\" ERROR: {error}\")\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Record performance data\n",
    "        interaction_data = {\n",
    "            \"template_id\": template_id,\n",
    "            \"successful\": successful,\n",
    "            \"error\": bool(error),\n",
    "            \"error_message\": error,\n",
    "            \"latency\": latency,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"tokens\": {\n",
    "                \"input\": len(filled_template) // 4,  # Rough estimate\n",
    "                \"output\": len(result) // 4 if result else 0  # Rough estimate\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        performance_analyzer.add_interaction(interaction_data)\n",
    "        \n",
    "        results.append({\n",
    "            \"test_case\": test_case,\n",
    "            \"result\": result,\n",
    "            \"successful\": successful,\n",
    "            \"latency\": latency,\n",
    "            \"error\": error\n",
    "        })\n",
    "    \n",
    "    # Calculate overall performance\n",
    "    success_count = sum(1 for r in results if r[\"successful\"])\n",
    "    success_rate = success_count / len(results) if results else 0\n",
    "    error_count = sum(1 for r in results if r[\"error\"])\n",
    "    error_rate = error_count / len(results) if results else 0\n",
    "    avg_latency = sum(r[\"latency\"] for r in results) / len(results) if results else 0\n",
    "    \n",
    "    print(f\"  Results: {success_count}/{len(results)} successful ({success_rate:.2%}), \" \n",
    "          f\"{error_count} errors, avg latency: {avg_latency:.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        \"template_id\": template_id,\n",
    "        \"success_rate\": success_rate,\n",
    "        \"error_rate\": error_rate,\n",
    "        \"avg_latency\": avg_latency,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "# Initialize the performance analyzer\n",
    "real_performance_analyzer = PerformanceAnalyzer()\n",
    "\n",
    "# Run tests if AI engine is available\n",
    "test_results = {}\n",
    "if ai_engine:\n",
    "    for template_id, test_cases in test_data.items():\n",
    "        test_results[template_id] = await test_template(\n",
    "            ai_engine, template_id, test_cases, real_performance_analyzer\n",
    "        )\n",
    "    \n",
    "    # Convert results to DataFrame for visualization\n",
    "    df = pd.DataFrame({\n",
    "        template_id: {\n",
    "            \"success_rate\": results[\"success_rate\"],\n",
    "            \"error_rate\": results[\"error_rate\"],\n",
    "            \"avg_latency\": results[\"avg_latency\"]\n",
    "        }\n",
    "        for template_id, results in test_results.items()\n",
    "    }).T\n",
    "    \n",
    "    # Display the metrics\n",
    "    print(\"\\nTemplate Performance with Real LLM API:\")\n",
    "    display(df)\n",
    "    \n",
    "    # Visualize the performance metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot success and error rates\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[[\"success_rate\", \"error_rate\"]].plot(kind=\"bar\", ax=plt.gca())\n",
    "    plt.title(\"Success and Error Rates by Template\")\n",
    "    plt.ylabel(\"Rate\")\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot average latency\n",
    "    plt.subplot(1, 2, 2)\n",
    "    df[\"avg_latency\"].plot(kind=\"bar\", ax=plt.gca(), color=\"green\")\n",
    "    plt.title(\"Average Latency by Template\")\n",
    "    plt.ylabel(\"Seconds\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nSkipping real API tests since no API keys are available.\")\n",
    "    # Create simulated test results for demo purposes\n",
    "    test_results = {\n",
    "        \"sentiment_analysis\": {\n",
    "            \"template_id\": \"sentiment_analysis\",\n",
    "            \"success_rate\": 0.65,\n",
    "            \"error_rate\": 0.1,\n",
    "            \"avg_latency\": 1.2\n",
    "        },\n",
    "        \"question_answering\": {\n",
    "            \"template_id\": \"question_answering\",\n",
    "            \"success_rate\": 0.8,\n",
    "            \"error_rate\": 0.05,\n",
    "            \"avg_latency\": 1.8\n",
    "        },\n",
    "        \"code_generation\": {\n",
    "            \"template_id\": \"code_generation\",\n",
    "            \"success_rate\": 0.5,\n",
    "            \"error_rate\": 0.2,\n",
    "            \"avg_latency\": 2.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add simulated interaction data to the performance analyzer\n",
    "    for template_id, metrics in test_results.items():\n",
    "        for i in range(10):  # 10 simulated interactions per template\n",
    "            real_performance_analyzer.add_interaction({\n",
    "                \"template_id\": template_id,\n",
    "                \"successful\": i < (10 * metrics[\"success_rate\"]),\n",
    "                \"error\": i < (10 * metrics[\"error_rate\"]),\n",
    "                \"latency\": metrics[\"avg_latency\"] + (i % 5) * 0.2,\n",
    "                \"timestamp\": time.time() - (i * 60),\n",
    "                \"tokens\": {\n",
    "                    \"input\": 50 + (i % 10) * 5,\n",
    "                    \"output\": 100 + (i % 20) * 10\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Print simulated results\n",
    "    print(\"\\nSimulated Template Performance:\")\n",
    "    df = pd.DataFrame({\n",
    "        template_id: {\n",
    "            \"success_rate\": metrics[\"success_rate\"],\n",
    "            \"error_rate\": metrics[\"error_rate\"],\n",
    "            \"avg_latency\": metrics[\"avg_latency\"]\n",
    "        }\n",
    "        for template_id, metrics in test_results.items()\n",
    "    }).T\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1284a9",
   "metadata": {},
   "source": [
    "## 5. Running the Optimization Workflow with Real Performance Data\n",
    "\n",
    "Now that we have real performance data, let's run the optimization workflow to improve our templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the AdaptiveLearningManager with our real performance data\n",
    "learning_manager = AdaptiveLearningManager(\n",
    "    performance_analyzer=real_performance_analyzer,\n",
    "    prompt_library=prompt_library,\n",
    "    config={\n",
    "        \"success_rate_threshold\": 0.7,\n",
    "        \"error_rate_threshold\": 0.15,\n",
    "        \"min_sample_size\": 4,\n",
    "        \"auto_optimize_prompts\": True,\n",
    "        \"optimization_strategy\": \"rule_based\"  # Use rule-based since it doesn't require additional API calls\n",
    "    },\n",
    "    ai_engine=ai_engine\n",
    ")\n",
    "\n",
    "# Store the initial state of templates\n",
    "initial_templates = {}\n",
    "for template_id in prompt_library.list_template_ids():\n",
    "    initial_templates[template_id] = prompt_library.get_template(template_id)\n",
    "\n",
    "# Identify underperforming prompts\n",
    "underperforming = learning_manager.identify_underperforming_prompts()\n",
    "\n",
    "print(\"Underperforming templates identified:\")\n",
    "for template_id, metrics in underperforming.items():\n",
    "    print(f\"- {template_id}:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.2f}\")\n",
    "    \n",
    "    template = prompt_library.get_template(template_id)\n",
    "    print(f\"  Current template: {template.user_prompt_template[:50]}...\")\n",
    "\n",
    "# Run the learning cycle with auto-optimization\n",
    "if underperforming:\n",
    "    print(\"\\nRunning optimization cycle...\")\n",
    "    cycle_results = await learning_manager.run_learning_cycle(auto_optimize=True)\n",
    "    \n",
    "    print(f\"\\nOptimization cycle completed (Cycle ID: {cycle_results.get('cycle_id')}):\")\n",
    "    print(f\"- Templates analyzed: {len(cycle_results.get('analyzed_prompts', []))}\")\n",
    "    print(f\"- Underperforming templates: {len(cycle_results.get('underperforming_prompts', []))}\")\n",
    "    print(f\"- Templates optimized: {len(cycle_results.get('optimized_prompts', []))}\")\n",
    "    \n",
    "    # Show before/after comparison\n",
    "    print(\"\\nOptimized Templates (Before/After):\")\n",
    "    for template_id in cycle_results.get('optimized_prompts', []):\n",
    "        initial = initial_templates.get(template_id)\n",
    "        current = prompt_library.get_template(template_id)\n",
    "        \n",
    "        print(f\"\\n{template_id} (v{initial.version} → v{current.version}):\")\n",
    "        print(f\"BEFORE: {initial.user_prompt_template}\")\n",
    "        print(f\"AFTER:  {current.user_prompt_template}\")\n",
    "        \n",
    "        # Show optimization metadata if available\n",
    "        if hasattr(current, 'optimization_source') and current.optimization_source:\n",
    "            print(f\"OPTIMIZATION SOURCE: {current.optimization_source}\")\n",
    "        \n",
    "        if hasattr(current, 'version_notes') and current.version_notes:\n",
    "            print(f\"VERSION NOTES: {current.version_notes}\")\n",
    "else:\n",
    "    print(\"No underperforming templates identified for optimization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1abb00",
   "metadata": {},
   "source": [
    "## 6. Testing Optimized Templates (A/B Comparison)\n",
    "\n",
    "If we've optimized templates, let's test them against the original versions to measure improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba22e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimized templates if we have any and if the AI Engine is available\n",
    "optimized_templates = []\n",
    "if 'cycle_results' in locals() and ai_engine:\n",
    "    optimized_templates = cycle_results.get('optimized_prompts', [])\n",
    "elif 'cycle_results' not in locals() and ai_engine:\n",
    "    print(\"No templates were optimized in the previous step.\")\n",
    "elif not ai_engine:\n",
    "    print(\"Skipping A/B testing since no AI engine is available.\")\n",
    "    # Simulate optimized templates for demo purposes\n",
    "    optimized_templates = [\"sentiment_analysis\", \"code_generation\"]\n",
    "\n",
    "if optimized_templates and ai_engine:\n",
    "    print(f\"Running A/B tests for {len(optimized_templates)} optimized templates...\")\n",
    "    \n",
    "    # Create a new performance analyzer for the optimized templates\n",
    "    optimized_performance_analyzer = PerformanceAnalyzer()\n",
    "    \n",
    "    # Test each optimized template\n",
    "    optimized_results = {}\n",
    "    for template_id in optimized_templates:\n",
    "        optimized_results[template_id] = await test_template(\n",
    "            ai_engine, template_id, test_data[template_id], optimized_performance_analyzer\n",
    "        )\n",
    "    \n",
    "    # Compare original vs optimized results\n",
    "    print(\"\\nA/B Test Results (Original vs. Optimized):\")\n",
    "    for template_id in optimized_templates:\n",
    "        original = test_results.get(template_id, {})\n",
    "        optimized = optimized_results.get(template_id, {})\n",
    "        \n",
    "        success_change = optimized.get('success_rate', 0) - original.get('success_rate', 0)\n",
    "        error_change = optimized.get('error_rate', 0) - original.get('error_rate', 0)\n",
    "        latency_change = optimized.get('avg_latency', 0) - original.get('avg_latency', 0)\n",
    "        \n",
    "        print(f\"\\n{template_id}:\")\n",
    "        print(f\"- Success Rate: {original.get('success_rate', 0):.2f} → {optimized.get('success_rate', 0):.2f} ({success_change:+.2f})\")\n",
    "        print(f\"- Error Rate: {original.get('error_rate', 0):.2f} → {optimized.get('error_rate', 0):.2f} ({error_change:+.2f})\")\n",
    "        print(f\"- Avg Latency: {original.get('avg_latency', 0):.2f}s → {optimized.get('avg_latency', 0):.2f}s ({latency_change:+.2f}s)\")\n",
    "    \n",
    "    # Visualize the A/B comparison\n",
    "    comparison_data = {}\n",
    "    for template_id in optimized_templates:\n",
    "        original = test_results.get(template_id, {})\n",
    "        optimized = optimized_results.get(template_id, {})\n",
    "        \n",
    "        comparison_data[template_id] = {\n",
    "            \"Original Success Rate\": original.get('success_rate', 0),\n",
    "            \"Optimized Success Rate\": optimized.get('success_rate', 0),\n",
    "            \"Original Error Rate\": original.get('error_rate', 0),\n",
    "            \"Optimized Error Rate\": optimized.get('error_rate', 0),\n",
    "            \"Original Latency\": original.get('avg_latency', 0),\n",
    "            \"Optimized Latency\": optimized.get('avg_latency', 0)\n",
    "        }\n",
    "    \n",
    "    if comparison_data:\n",
    "        # Convert to DataFrame and plot\n",
    "        comparison_df = pd.DataFrame(comparison_data).T\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Plot success rate comparison\n",
    "        plt.subplot(1, 3, 1)\n",
    "        comparison_df[[\"Original Success Rate\", \"Optimized Success Rate\"]].plot(kind=\"bar\", ax=plt.gca())\n",
    "        plt.title(\"Success Rate Comparison\")\n",
    "        plt.ylabel(\"Rate\")\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Plot error rate comparison\n",
    "        plt.subplot(1, 3, 2)\n",
    "        comparison_df[[\"Original Error Rate\", \"Optimized Error Rate\"]].plot(kind=\"bar\", ax=plt.gca())\n",
    "        plt.title(\"Error Rate Comparison\")\n",
    "        plt.ylabel(\"Rate\")\n",
    "        plt.ylim(0, max(0.5, comparison_df[\"Original Error Rate\"].max() * 1.2))\n",
    "        \n",
    "        # Plot latency comparison\n",
    "        plt.subplot(1, 3, 3)\n",
    "        comparison_df[[\"Original Latency\", \"Optimized Latency\"]].plot(kind=\"bar\", ax=plt.gca())\n",
    "        plt.title(\"Latency Comparison\")\n",
    "        plt.ylabel(\"Seconds\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "elif optimized_templates:\n",
    "    # Create simulated A/B test results for demo purposes\n",
    "    print(\"\\nSimulated A/B Test Results (Original vs. Optimized):\")\n",
    "    \n",
    "    for template_id in optimized_templates:\n",
    "        original = test_results.get(template_id, {})\n",
    "        # Simulate improvements\n",
    "        optimized_success_rate = min(1.0, original.get('success_rate', 0.5) * 1.2)  # 20% improvement\n",
    "        optimized_error_rate = max(0, original.get('error_rate', 0.2) * 0.8)  # 20% reduction\n",
    "        \n",
    "        success_change = optimized_success_rate - original.get('success_rate', 0)\n",
    "        error_change = optimized_error_rate - original.get('error_rate', 0)\n",
    "        \n",
    "        print(f\"\\n{template_id}:\")\n",
    "        print(f\"- Success Rate: {original.get('success_rate', 0):.2f} → {optimized_success_rate:.2f} ({success_change:+.2f})\")\n",
    "        print(f\"- Error Rate: {original.get('error_rate', 0):.2f} → {optimized_error_rate:.2f} ({error_change:+.2f})\")\n",
    "    \n",
    "    # Create simulated visualization data\n",
    "    comparison_data = {}\n",
    "    for template_id in optimized_templates:\n",
    "        original = test_results.get(template_id, {})\n",
    "        comparison_data[template_id] = {\n",
    "            \"Original Success Rate\": original.get('success_rate', 0.5),\n",
    "            \"Optimized Success Rate\": min(1.0, original.get('success_rate', 0.5) * 1.2),\n",
    "            \"Original Error Rate\": original.get('error_rate', 0.2),\n",
    "            \"Optimized Error Rate\": max(0, original.get('error_rate', 0.2) * 0.8)\n",
    "        }\n",
    "    \n",
    "    # Plot simulated results\n",
    "    comparison_df = pd.DataFrame(comparison_data).T\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    comparison_df[[\"Original Success Rate\", \"Optimized Success Rate\"]].plot(kind=\"bar\", ax=plt.gca())\n",
    "    plt.title(\"Simulated Success Rate Comparison\")\n",
    "    plt.ylabel(\"Rate\")\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    comparison_df[[\"Original Error Rate\", \"Optimized Error Rate\"]].plot(kind=\"bar\", ax=plt.gca())\n",
    "    plt.title(\"Simulated Error Rate Comparison\")\n",
    "    plt.ylabel(\"Rate\")\n",
    "    plt.ylim(0, max(0.5, comparison_df[\"Original Error Rate\"].max() * 1.2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202b549",
   "metadata": {},
   "source": [
    "## 7. Cost-Performance Analysis\n",
    "\n",
    "Let's analyze the cost-performance tradeoffs of our prompt optimization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define token cost estimates for different models (per 1K tokens)\n",
    "token_costs = {\n",
    "    \"gpt-4o-mini\": {\"input\": 0.005, \"output\": 0.015},\n",
    "    \"gpt-4o\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"claude-3-haiku-20240307\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "    \"claude-3-sonnet-20240229\": {\"input\": 0.003, \"output\": 0.015}\n",
    "}\n",
    "\n",
    "# Function to estimate costs\n",
    "def estimate_cost(model, input_tokens, output_tokens):\n",
    "    \"\"\"Estimate API cost for tokens.\"\"\"\n",
    "    if model not in token_costs:\n",
    "        return 0\n",
    "        \n",
    "    input_cost = (input_tokens / 1000) * token_costs[model][\"input\"]\n",
    "    output_cost = (output_tokens / 1000) * token_costs[model][\"output\"]\n",
    "    return input_cost + output_cost\n",
    "\n",
    "# Calculate costs for our test cases\n",
    "if ai_engine:\n",
    "    current_model = ai_engine.config[\"providers\"][ai_engine.config[\"default_provider\"]][\"default_model\"]\n",
    "else:\n",
    "    # Use a default model for simulation\n",
    "    current_model = \"gpt-4o-mini\" if \"openai\" in api_keys_available and api_keys_available[\"openai\"] else \"claude-3-haiku-20240307\"\n",
    "\n",
    "# Calculate average tokens per template\n",
    "if not 'optimized_results' in locals():\n",
    "    # Create simulated data for demonstration\n",
    "    optimized_results = {k: {\"avg_tokens\": {\"input\": 200, \"output\": 400}} for k in test_results.keys()}\n",
    "\n",
    "template_tokens = {}\n",
    "for template_id in test_results.keys():\n",
    "    # Note: In a real scenario, you would get these from actual API responses\n",
    "    # For demo purposes, we'll estimate based on template complexity\n",
    "    template = prompt_library.get_template(template_id)\n",
    "    avg_input_tokens = len(template.user_prompt_template) // 4\n",
    "    \n",
    "    if template_id == \"sentiment_analysis\":\n",
    "        avg_output_tokens = 20  # Short responses\n",
    "    elif template_id == \"question_answering\":\n",
    "        avg_output_tokens = 100  # Medium responses\n",
    "    else:  # code_generation\n",
    "        avg_output_tokens = 300  # Long responses\n",
    "        \n",
    "    template_tokens[template_id] = {\n",
    "        \"input\": avg_input_tokens,\n",
    "        \"output\": avg_output_tokens\n",
    "    }\n",
    "\n",
    "# Calculate costs per 1000 API calls\n",
    "cost_analysis = {}\n",
    "for template_id, tokens in template_tokens.items():\n",
    "    # Cost before optimization\n",
    "    original_success_rate = test_results.get(template_id, {}).get('success_rate', 0.7)\n",
    "    original_cost_per_call = estimate_cost(current_model, tokens[\"input\"], tokens[\"output\"])\n",
    "    original_calls_needed = 1000 / original_success_rate if original_success_rate > 0 else float('inf')\n",
    "    original_total_cost = original_cost_per_call * original_calls_needed\n",
    "    \n",
    "    # Cost after optimization (if available)\n",
    "    if template_id in optimized_templates:\n",
    "        optimized_success_rate = (\n",
    "            optimized_results.get(template_id, {}).get('success_rate', 0)\n",
    "            if 'optimized_results' in locals() else\n",
    "            min(1.0, original_success_rate * 1.2)  # Simulate 20% improvement\n",
    "        )\n",
    "        # Tokens might be slightly higher for optimized prompts\n",
    "        optimized_tokens = {\n",
    "            \"input\": tokens[\"input\"] * 1.1,  # Assume 10% more tokens in optimized prompts\n",
    "            \"output\": tokens[\"output\"]\n",
    "        }\n",
    "        optimized_cost_per_call = estimate_cost(current_model, optimized_tokens[\"input\"], optimized_tokens[\"output\"])\n",
    "        optimized_calls_needed = 1000 / optimized_success_rate if optimized_success_rate > 0 else float('inf')\n",
    "        optimized_total_cost = optimized_cost_per_call * optimized_calls_needed\n",
    "    else:\n",
    "        optimized_success_rate = original_success_rate\n",
    "        optimized_cost_per_call = original_cost_per_call\n",
    "        optimized_calls_needed = original_calls_needed\n",
    "        optimized_total_cost = original_total_cost\n",
    "    \n",
    "    # Calculate the return on investment (ROI)\n",
    "    cost_savings = original_total_cost - optimized_total_cost\n",
    "    optimization_cost = estimate_cost(current_model, 500, 1000)  # Rough estimate for running optimization\n",
    "    roi = (cost_savings / optimization_cost) if optimization_cost > 0 else 0\n",
    "    \n",
    "    cost_analysis[template_id] = {\n",
    "        \"original_success_rate\": original_success_rate,\n",
    "        \"optimized_success_rate\": optimized_success_rate,\n",
    "        \"original_cost_per_call\": original_cost_per_call,\n",
    "        \"optimized_cost_per_call\": optimized_cost_per_call,\n",
    "        \"original_calls_needed\": original_calls_needed,\n",
    "        \"optimized_calls_needed\": optimized_calls_needed,\n",
    "        \"original_total_cost\": original_total_cost,\n",
    "        \"optimized_total_cost\": optimized_total_cost,\n",
    "        \"cost_savings\": cost_savings,\n",
    "        \"optimization_cost\": optimization_cost,\n",
    "        \"roi\": roi\n",
    "    }\n",
    "\n",
    "# Display the cost analysis\n",
    "print(f\"Cost Analysis for {current_model}:\")\n",
    "print(f\"Estimated costs for 1000 successful API calls:\")\n",
    "\n",
    "cost_df = pd.DataFrame({\n",
    "    template_id: {\n",
    "        \"Original Success Rate\": f\"{data['original_success_rate']:.2%}\",\n",
    "        \"Optimized Success Rate\": f\"{data['optimized_success_rate']:.2%}\",\n",
    "        \"Original Cost\": f\"${data['original_total_cost']:.2f}\",\n",
    "        \"Optimized Cost\": f\"${data['optimized_total_cost']:.2f}\",\n",
    "        \"Cost Savings\": f\"${data['cost_savings']:.2f}\",\n",
    "        \"ROI\": f\"{data['roi']:.1f}x\"\n",
    "    }\n",
    "    for template_id, data in cost_analysis.items()\n",
    "}).T\n",
    "\n",
    "display(cost_df)\n",
    "\n",
    "# Create visualization of cost savings\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot costs\n",
    "plt.subplot(1, 2, 1)\n",
    "cost_comparison = pd.DataFrame({\n",
    "    template_id: {\n",
    "        \"Original\": data[\"original_total_cost\"],\n",
    "        \"Optimized\": data[\"optimized_total_cost\"]\n",
    "    }\n",
    "    for template_id, data in cost_analysis.items()\n",
    "}).T\n",
    "cost_comparison.plot(kind=\"bar\", ax=plt.gca())\n",
    "plt.title(f\"Cost for 1000 Successful Calls ({current_model})\")\n",
    "plt.ylabel(\"Cost ($)\")\n",
    "\n",
    "# Plot ROI\n",
    "plt.subplot(1, 2, 2)\n",
    "roi_data = [data[\"roi\"] for data in cost_analysis.values()]\n",
    "plt.bar(list(cost_analysis.keys()), roi_data)\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', label=\"Break-even (ROI = 1)\")\n",
    "plt.title(\"Return on Investment (ROI)\")\n",
    "plt.ylabel(\"ROI (x)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf479f3",
   "metadata": {},
   "source": [
    "## 8. Summary and Best Practices\n",
    "\n",
    "In this notebook, we've demonstrated how to integrate the prompt optimization workflow with real LLM APIs. We've seen:\n",
    "\n",
    "1. **Real API Integration**: How to connect to OpenAI and Anthropic APIs for testing\n",
    "2. **Performance Testing**: How to evaluate prompt templates with real-world test cases\n",
    "3. **Optimization Workflow**: Running the optimization cycle with real performance data\n",
    "4. **A/B Testing**: Comparing original and optimized templates to measure improvement\n",
    "5. **Cost Analysis**: Analyzing the cost-performance tradeoffs of prompt optimization\n",
    "\n",
    "### Best Practices for Prompt Optimization with Real APIs:\n",
    "\n",
    "1. **Maintain a Test Suite**: Keep a diverse set of test cases that represent real-world usage\n",
    "2. **Track Multiple Metrics**: Don't just track success rate; also consider error rate, latency, and cost\n",
    "3. **Cost-Aware Optimization**: Balance performance improvements against token usage increases\n",
    "4. **Incremental Optimization**: Make small, targeted improvements rather than complete rewrites\n",
    "5. **Regular Re-evaluation**: Continuously test optimized prompts against new test cases\n",
    "6. **Version Control**: Maintain clear version history for all templates\n",
    "7. **Context Preservation**: Ensure optimizations maintain the original intent and functionality\n",
    "\n",
    "By following these practices, you can build a robust prompt optimization pipeline that continuously improves your LLM applications while managing costs effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
